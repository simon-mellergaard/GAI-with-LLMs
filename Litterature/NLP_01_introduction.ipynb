{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/NLP_01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/simonmellergaard/nlp-01-introduction/edit/run/260666921)","metadata":{"id":"6oa2bmgTx4pO"}},{"cell_type":"markdown","source":"A small little change","metadata":{}},{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n!git clone https://github.com/nlp-with-transformers/notebooks.git\n%cd notebooks\nfrom install import *\ninstall_requirements()","metadata":{"id":"wyHmsvoa8ih9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide\nfrom utils import *\nsetup_chapter()","metadata":{"id":"YS1UJq8Z8ih_"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hello Transformers","metadata":{"id":"zAo_pjd08iiA"}},{"cell_type":"markdown","source":"<img alt=\"transformer-timeline\" caption=\"The transformers timeline\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_timeline.png?raw=1\" id=\"transformer-timeline\"/>","metadata":{"id":"qKhTz_Uw8iiB"}},{"cell_type":"markdown","source":"## The Encoder-Decoder Framework","metadata":{"id":"j4gCZJFI8iiB"}},{"cell_type":"markdown","source":"<img alt=\"rnn\" caption=\"Unrolling an RNN in time.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_rnn.png?raw=1\" id=\"rnn\"/>","metadata":{"id":"nuF_-n7R8iiC"}},{"cell_type":"markdown","source":"<img alt=\"enc-dec\" caption=\"Encoder-decoder architecture with a pair of RNNs. In general, there are many more recurrent layers than those shown.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_enc-dec.png?raw=1\" id=\"enc-dec\"/>","metadata":{"id":"lPSKzWRx8iiD"}},{"cell_type":"markdown","source":"## Attention Mechanisms","metadata":{"id":"suShx5Ex8iiE"}},{"cell_type":"markdown","source":"<img alt=\"enc-dec-attn\" caption=\"Encoder-decoder architecture with an attention mechanism for a pair of RNNs.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_enc-dec-attn.png?raw=1\" id=\"enc-dec-attn\"/>","metadata":{"id":"eYMF8Z1h8iiE"}},{"cell_type":"markdown","source":"<img alt=\"attention-alignment\" width=\"500\" caption=\"RNN encoder-decoder alignment of words in English and the generated translation in French (courtesy of Dzmitry Bahdanau).\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter02_attention-alignment.png?raw=1\" id=\"attention-alignment\"/>","metadata":{"id":"b9_0K6nb8iiF"}},{"cell_type":"markdown","source":"<img alt=\"transformer-self-attn\" caption=\"Encoder-decoder architecture of the original Transformer.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_self-attention.png?raw=1\" id=\"transformer-self-attn\"/>","metadata":{"id":"Bjmo8ebh8iiP"}},{"cell_type":"markdown","source":"## Transfer Learning in NLP","metadata":{"id":"88kkvvlQ8iiP"}},{"cell_type":"markdown","source":"<img alt=\"transfer-learning\" caption=\"Comparison of traditional supervised learning (left) and transfer learning (right).\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_transfer-learning.png?raw=1\" id=\"transfer-learning\"/>  ","metadata":{"id":"pOW2ihG58iiP"}},{"cell_type":"markdown","source":"<img alt=\"ulmfit\" width=\"500\" caption=\"The ULMFiT process (courtesy of Jeremy Howard).\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_ulmfit.png?raw=1\" id=\"ulmfit\"/>","metadata":{"id":"Cb10n7DK8iiQ"}},{"cell_type":"markdown","source":"## Hugging Face Transformers: Bridging the Gap","metadata":{"id":"GnCmz6tW8iiQ"}},{"cell_type":"markdown","source":"## A Tour of Transformer Applications","metadata":{"id":"hyPwyeGs8iiQ"}},{"cell_type":"code","source":"text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\nfrom your online store in Germany. Unfortunately, when I opened the package, \\\nI discovered to my horror that I had been sent an action figure of Megatron \\\ninstead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\ndilemma. To resolve the issue, I demand an exchange of Megatron for the \\\nOptimus Prime figure I ordered. Enclosed are copies of my records concerning \\\nthis purchase. I expect to hear from you soon. Sincerely, Bumblebee. \"\"\"","metadata":{"id":"sXR5tiXr8iiQ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text Classification","metadata":{"id":"M61zfWda8iiR"}},{"cell_type":"code","source":"#hide_output\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\")","metadata":{"id":"rHPhFZJh8iiR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\noutputs = classifier(text)\npd.DataFrame(outputs)","metadata":{"id":"XZBPXAM58iiR"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Named Entity Recognition","metadata":{"id":"QPtjU0G-8iiS"}},{"cell_type":"code","source":"ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\noutputs = ner_tagger(text)\npd.DataFrame(outputs)","metadata":{"id":"8CC0CMdl8iiS"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Question Answering","metadata":{"id":"pNs284yE8iiS"}},{"cell_type":"code","source":"reader = pipeline(\"question-answering\")\nquestion = \"What does the customer want?\"\noutputs = reader(question=question, context=text)\npd.DataFrame([outputs])","metadata":{"id":"0EnxmSgw8iiT"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Summarization","metadata":{"id":"8EF7cJzA8iiT"}},{"cell_type":"code","source":"summarizer = pipeline(\"summarization\")\noutputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\nprint(outputs[0]['summary_text'])","metadata":{"id":"EyWyu-UY8iiU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Translation","metadata":{"id":"h8tQeqFv8iiU"}},{"cell_type":"code","source":"translator = pipeline(\"translation_en_to_de\",\n                      model=\"Helsinki-NLP/opus-mt-en-de\")\noutputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\nprint(outputs[0]['translation_text'])","metadata":{"id":"yF-XPMgX8iiU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text Generation","metadata":{"id":"lZhZJKR98iiV"}},{"cell_type":"code","source":"#hide\nfrom transformers import set_seed\nset_seed(42) # Set the seed to get reproducible results","metadata":{"id":"9udKYLyU8iiV"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = pipeline(\"text-generation\")\nresponse = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt = text + \"\\n\\nCustomer service response:\\n\" + response\noutputs = generator(prompt, max_length=200)\nprint(outputs[0]['generated_text'])","metadata":{"id":"TDAhhEll8iiV"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Hugging Face Ecosystem","metadata":{"id":"ed5Wdz9b8iiV"}},{"cell_type":"markdown","source":"<img alt=\"ecosystem\" width=\"500\" caption=\"An overview of the Hugging Face ecosystem of libraries and the Hub.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_hf-ecosystem.png?raw=1\" id=\"ecosystem\"/>","metadata":{"id":"SHYJ06gj8iiV"}},{"cell_type":"markdown","source":"### The Hugging Face Hub","metadata":{"id":"d1Ih56l98iiW"}},{"cell_type":"markdown","source":"<img alt=\"hub-overview\" width=\"1000\" caption=\"The models page of the Hugging Face Hub, showing filters on the left and a list of models on the right.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_hub-overview.png?raw=1\" id=\"hub-overview\"/>","metadata":{"id":"pnZuFV7Z8iiW"}},{"cell_type":"markdown","source":"<img alt=\"hub-model-card\" width=\"1000\" caption=\"A example model card from the Hugging Face Hub. The inference widget is shown on the right, where you can interact with the model.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_hub-model-card.png?raw=1\" id=\"hub-model-card\"/>","metadata":{"id":"wAE8kJUh8iiW"}},{"cell_type":"markdown","source":"### Hugging Face Tokenizers","metadata":{"id":"97ARTVHr8iiW"}},{"cell_type":"markdown","source":"### Hugging Face Datasets","metadata":{"id":"rD4kfyhN8iiW"}},{"cell_type":"markdown","source":"### Hugging Face Accelerate","metadata":{"id":"K2CCEzKY8iiW"}},{"cell_type":"markdown","source":"## Main Challenges with Transformers","metadata":{"id":"1xgxxAEZ8iia"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"sCXlU6Ky8iia"}}]}