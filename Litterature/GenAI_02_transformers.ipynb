{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/GenAI_02_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/simonmellergaard/genai-02-transformers/edit)"
      ],
      "metadata": {
        "id": "kuquSYtOpsOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers"
      ],
      "metadata": {
        "id": "5zQ9K-Ms-8Qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a supplement to the Transformers Chapter of the [Hands-On Generative AI with Transformers and Diffusion Models](https://learning.oreilly.com/library/view/hands-on-generative-ai/9781098149239/) book. This notebooks includes:\n",
        "\n",
        "* The code from the book\n",
        "* Additional examples\n",
        "* Exercise solutions."
      ],
      "metadata": {
        "id": "21BxHPqj-8Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "kS_un8Xh-8Qu"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Language Model in Action"
      ],
      "metadata": {
        "id": "vUxxHKdz-8Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing Text"
      ],
      "metadata": {
        "id": "DZlWCYrf-8Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use the id of the model you want to use\n",
        "# GPT-2 \"openai-community/gpt2\"\n",
        "# Qwen \"Qwen/Qwen2-0.5B\"\n",
        "# SmolLM \"HuggingFaceTB/SmolLM-135M\"\n",
        "\n",
        "prompt = \"It was a dark and stormy\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "input_ids = tokenizer(prompt).input_ids\n",
        "input_ids"
      ],
      "metadata": {
        "id": "lSaZ_SMn-8Qw",
        "outputId": "a150923c-c5fd-4e28-ceb4-08c7665debac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2132, 572, 264, 6319, 323, 13458, 88]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "for t in input_ids:\n",
        "    print(t, \"\\t:\", tokenizer.decode(t))"
      ],
      "metadata": {
        "id": "VebQiCMj-8Qx",
        "outputId": "d9ec9f8b-dc69-441a-d49e-e8db0a6fa06a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2132 \t: It\n",
            "572 \t:  was\n",
            "264 \t:  a\n",
            "6319 \t:  dark\n",
            "323 \t:  and\n",
            "13458 \t:  storm\n",
            "88 \t: y\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting Probabilities"
      ],
      "metadata": {
        "id": "pi708T6f-8Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")"
      ],
      "metadata": {
        "id": "TwtH0F_T-8Qy"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "# We tokenize again but specifying the tokenizer that we want it to\n",
        "# return a PyTorch tensor, which is what the model expects,\n",
        "# rather than a list of integers\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model(input_ids)\n",
        "outputs.logits.shape  # An output for each input token"
      ],
      "metadata": {
        "id": "QAlZI7u7-8Qy",
        "outputId": "7f7b3bb0-5428-4148-b145-f768f53dec60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 151936])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "final_logits = model(input_ids).logits[0, -1]  # The last set of logits\n",
        "final_logits.argmax()  # The position of the maximum"
      ],
      "metadata": {
        "id": "lNLY-wAS-8Q0",
        "outputId": "f4d04f6e-61d8-471a-e487-b3d4abbc775f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3729)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(final_logits.argmax())"
      ],
      "metadata": {
        "id": "B8-qsw_k-8Q0",
        "outputId": "61abcebb-b509-46d8-9bd2-037e73fcfae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' night'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "top10_logits = torch.topk(final_logits, 10)\n",
        "for index in top10_logits.indices:\n",
        "    print(tokenizer.decode(index))"
      ],
      "metadata": {
        "id": "oFE0HBB8-8Q0",
        "outputId": "6f17a6ec-709b-44ff-d4e5-b23e64ffa97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " night\n",
            " evening\n",
            " day\n",
            " morning\n",
            " winter\n",
            " afternoon\n",
            " Saturday\n",
            " Sunday\n",
            " Friday\n",
            " October\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "top10 = torch.topk(final_logits.softmax(dim=0), 10)\n",
        "for value, index in zip(top10.values, top10.indices):\n",
        "    print(f\"{tokenizer.decode(index):<10} {value.item():.2%}\")"
      ],
      "metadata": {
        "id": "6Vhc0ixi-8Q0",
        "outputId": "ce8d6240-4695-4f04-b693-3b3e5aba928b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " night     88.71%\n",
            " evening   4.30%\n",
            " day       2.19%\n",
            " morning   0.49%\n",
            " winter    0.45%\n",
            " afternoon 0.27%\n",
            " Saturday  0.25%\n",
            " Sunday    0.19%\n",
            " Friday    0.17%\n",
            " October   0.16%\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Text"
      ],
      "metadata": {
        "id": "lGr55H3U-8Q1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Decoding**"
      ],
      "metadata": {
        "id": "9MtMrKQr-8Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model.generate(input_ids, max_new_tokens=20)\n",
        "decoded_text = tokenizer.decode(output_ids[0])\n",
        "\n",
        "print(\"Input IDs\", input_ids[0])\n",
        "print(\"Output IDs\", output_ids)\n",
        "print(f\"Generated text: {decoded_text}\")"
      ],
      "metadata": {
        "id": "vMuRXKHV-8Q1",
        "outputId": "ca270006-b418-4005-82a5-25f2d20f92cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs tensor([ 2132,   572,   264,  6319,   323, 13458,    88])\n",
            "Output IDs tensor([[ 2132,   572,   264,  6319,   323, 13458,    88,  3729,    13,   576,\n",
            "         12884,   572,  6319,   323,   279,  9956,   572,  1246,  2718,    13,\n",
            "           576, 11174,   572, 50413,  1495,   323,   279]])\n",
            "Generated text: It was a dark and stormy night. The sky was dark and the wind was howling. The rain was pouring down and the\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam Search**"
      ],
      "metadata": {
        "id": "aAAHnUdp-8Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=5,\n",
        "    max_new_tokens=30,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ],
      "metadata": {
        "id": "hpp7pS5O-8Q1",
        "outputId": "3498b505-4542-4863-d413-71b77ce12adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night. The wind was howling, and the rain was pouring down. The sky was dark and gloomy, and the air was filled with the\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam search with repetition penalty**"
      ],
      "metadata": {
        "id": "EQ3Yv1M4-8Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=5,\n",
        "    repetition_penalty=2.0,\n",
        "    max_new_tokens=38,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ],
      "metadata": {
        "id": "9_8PRF1Y-8Q2",
        "outputId": "6e2c5941-144f-4f71-adca-81e0b44d31a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night. The sky was filled with thunder and lightning, and the wind howled in the distance. It was raining cats and dogs, and the streets were covered in puddles of water.\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling**"
      ],
      "metadata": {
        "id": "r0VKCNvQ-8Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "# Setting the seed ensures we get the same results every time we run this code\n",
        "set_seed(70)\n",
        "\n",
        "sampling_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=34,\n",
        "    top_k=0,  # We'll come back to this parameter\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sampling_output[0]))"
      ],
      "metadata": {
        "id": "TIXO8zTZ-8Q2",
        "outputId": "79cb4a2d-565f-414b-e125-95c4f7b2c968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night.Six pots of moisture laced the sky，pulling back fog and issuing a shared smell.\n",
            "16 km away they were cleaned and brought to her attention by\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling with different `temperature`s**"
      ],
      "metadata": {
        "id": "k8cCSOfL-8Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    max_new_tokens=40,\n",
        "    top_k=0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sampling_output[0]))"
      ],
      "metadata": {
        "id": "3If19lAV-8Q3",
        "outputId": "076f7583-cc8c-4746-ab92-b022eeb1723f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night in the hills of the Sierra Madre. The rain was heavy and the wind howled. The thunder was deafening and the lightning was a sight to behold. The sky was filled with clouds\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.001,\n",
        "    max_new_tokens=40,\n",
        "    top_k=0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sampling_output[0]))"
      ],
      "metadata": {
        "id": "h4jMutKr-8Q3",
        "outputId": "b0c3cf99-1a83-40c3-9783-f5fa11cc76c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night. The sky was dark and the wind was howling. The rain was pouring down and the lightning was flashing. The sky was dark and the wind was howling. The rain was pouring down\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=3.0,\n",
        "    max_new_tokens=40,\n",
        "    top_k=0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sampling_output[0]))"
      ],
      "metadata": {
        "id": "tnbPXJSL-8Q3",
        "outputId": "71762820-c5a3-45ec-c98b-028201daf707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy wheahkan exhilar swords seasHe Bd HibernateOthers Турية freed deploy Exhibition strtotimering finishing invadingmarker honoringЩ Uniform barracks Joan onde abbrev Mg/get铟 railway sticking Ant municipalities Kgforeach covering kin grown tacticalButtonText\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling with `top_k`**"
      ],
      "metadata": {
        "id": "26lQ3kXP-8Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=40,\n",
        "    top_k=10,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sampling_output[0]))"
      ],
      "metadata": {
        "id": "l40R-MO7-8Q4",
        "outputId": "042689be-28c1-47b0-e77f-0bb4c5c487f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night in the small town of Ketchum in 1989. It had been a cold and wet January. A group of 33 college students were gathered to go hunting for deer\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top-p sampling**"
      ],
      "metadata": {
        "id": "iaXe14Qn-8Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=40,\n",
        "    top_p=0.94,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sampling_output[0]))"
      ],
      "metadata": {
        "id": "hUF0ATNn-8Q4",
        "outputId": "140f2890-6ecb-4afa-ab9c-f8b9bebe4d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night in early July 2017. Despite the stormy weather the 2017 edition of the International FFA National High School Competition was successful, as 12 teams from\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot Classification"
      ],
      "metadata": {
        "id": "1jQ_3TNH-8Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Book code"
      ],
      "metadata": {
        "id": "P9YtN4NR-8Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the token IDs for the words ' positive' and ' negative'\n",
        "# (note the space before the words)\n",
        "tokenizer.encode(\" positive\"), tokenizer.encode(\" negative\")"
      ],
      "metadata": {
        "id": "BYigWFml-8Q4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def score(review):\n",
        "    \"\"\"Predict whether it is positive or negative\n",
        "\n",
        "    This function predicts whether a review is positive or negative\n",
        "    using a bit of clever prompting. It looks at the logits for the\n",
        "    tokens ' positive' and ' negative', and returns the label\n",
        "    with the highest score.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Question: Is the following review positive or\n",
        "negative about the movie?\n",
        "Review: {review} Answer:\"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids  # <1>\n",
        "    final_logits = model(input_ids).logits[0, -1]  # <2>\n",
        "    if final_logits[6785] > final_logits[8225]:  # <3>\n",
        "        print(\"Positive\")\n",
        "    else:\n",
        "        print(\"Negative\")"
      ],
      "metadata": {
        "id": "SgmAz4K3-8Q5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "score(\"This movie was terrible!\")"
      ],
      "metadata": {
        "id": "pBoJZtmv-8Q5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "score(\"That movie was great!\")"
      ],
      "metadata": {
        "id": "syEhQvqY-8Q5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "score(\"A complex yet wonderful film about the depravity of man\")  # A mistake"
      ],
      "metadata": {
        "id": "gwf-WopS-8Q7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Suplementary material (not in the book)"
      ],
      "metadata": {
        "id": "BGv8Oy8--8Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section shows how to use a pre-trained generative model to perform classification via zero-shot classification. We'll use a dataset of labeled reviews and measure the confusion matrix. A confusion matrix serves as a table summarizing a model's performance, depicting counts of true positive, true negative, false positive, and false negative predictions. Rows indicate actual (ground truth) classes, while columns indicate predicted classes. Analyzing this matrix provides insights into the model's strengths and weaknesses in distinguishing between specific classes."
      ],
      "metadata": {
        "id": "OXJiYnB4-8Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the `Qwen/Qwen2-0.5B` model and its tokenizer."
      ],
      "metadata": {
        "id": "GYgnDUG_-8Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from genaibook.core import get_device\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = get_device()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\").to(device)"
      ],
      "metadata": {
        "id": "3wxCxU5s-8RA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define our `score` function just as in the book, but with some key differences:\n",
        "\n",
        "* we use 1 (`positive`) and 0 (`negative`),\n",
        "* we receive a sample so we can use the `datasets` library. You can use this as a dictionary with the review being accessible with `sample[\"text\"]`,\n",
        "* we add a new column, `pred`, which contains the 0 or 1 from the model.\n",
        "* we add `truncation=True` to the `tokenizer` call. Truncation is needed as `score` will be called with a batch of samples, and we need to ensure that all samples have the same length. We'll explain in future chapters why this is needed, but for now, just remember that it's necessary when working with batches of samples."
      ],
      "metadata": {
        "id": "vtsJ4dGS-8RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\" positive\"), tokenizer.encode(\" negative\")"
      ],
      "metadata": {
        "id": "UatsrFsw-8RA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def score(sample):\n",
        "    \"\"\"Given a review, predict whether it is positive or negative using a bit of clever prompting\"\"\"\n",
        "    prompt = f\"Question: Is the following review positive or negative about the movie?\\nReview: {sample['text']} Answer:\"\n",
        "    input_ids = tokenizer(\n",
        "        prompt, truncation=True, return_tensors=\"pt\"\n",
        "    ).input_ids.to(device)\n",
        "    final_logits = model(input_ids).logits[0, -1]\n",
        "    if final_logits[6785] > final_logits[8225]:\n",
        "        sample[\"pred\"] = 1\n",
        "    else:\n",
        "        sample[\"pred\"] = 0\n",
        "    return sample"
      ],
      "metadata": {
        "id": "vRNzr80B-8RA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "score({\"text\": \"This movie was terrible!\"})"
      ],
      "metadata": {
        "id": "nthla9Mo-8RA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the [IMDB](https://huggingface.co/datasets/imdb) dataset, which contains 25,000 rows of labeled reviews. We'll use the `datasets` library to load the dataset and load the `train` split."
      ],
      "metadata": {
        "id": "mOot17Hh-8RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")[\"train\"]"
      ],
      "metadata": {
        "id": "oLFXT34u-8RA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fast iteration, we'll just use 1000 random samples from the dataset. We'll `shuffle` the dataset and then take the first 1000 samples."
      ],
      "metadata": {
        "id": "UNKSOWZ4-8RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "small_dataset = shuffled_dataset.select(range(1000))"
      ],
      "metadata": {
        "id": "GtH2NIV2-8RB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can use the `score` function to predict the sentiment of the reviews and compare the predictions with the actual labels."
      ],
      "metadata": {
        "id": "H-9_YdtW-8RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_dataset = small_dataset.map(score)\n",
        "updated_dataset"
      ],
      "metadata": {
        "id": "_1mm_IYz-8RB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the `evaluate` library to obtain the confusion matrix. A confusion matrix serves as a table summarizing a model's performance, depicing counts of true positive, true negatives, false positive, and false negative predictions. Rows indicate actual (label) classes, while columns indicate predicted classes. Analyzing this matrix provides insights into the model's strengths and weaknesses in distinguishing between classes. The fine-tune LLM chapter dives into this metric."
      ],
      "metadata": {
        "id": "Y5LMJHdW-8RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "confusion_matrix = evaluate.load(\"confusion_matrix\")\n",
        "cm = confusion_matrix.compute(\n",
        "    references=updated_dataset[\"label\"],\n",
        "    predictions=updated_dataset[\"pred\"],\n",
        ")\n",
        "cm"
      ],
      "metadata": {
        "id": "Hztz-R2--8RB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm[\"confusion_matrix\"],\n",
        "    display_labels=[\"negative\", \"positive\"],\n",
        ")\n",
        "disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "\n",
        "plt.title(\"Normalized confusion matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3wn4vW2H-8RC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do we get out of these results?\n",
        "\n",
        "* The model does not perform amazingly well. There are many false negatives.\n",
        "* The model is very good identifying most negative reviews, but unfortunately marks many positive reviews as negative.\n",
        "\n",
        "Finally, we can also convert the dataset to a Pandas `DataFrame` and explore the data directly. Here is a starting point"
      ],
      "metadata": {
        "id": "RkhmRDxD-8RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = updated_dataset.to_pandas()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "363pZ2wL-8RC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now it's your turn**. Experiment with tweaking the prompt to improve the model performance. What's the best confusion matrix you can achieve? What if you try another model?"
      ],
      "metadata": {
        "id": "_NKoT01p-8RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-Shot Generation"
      ],
      "metadata": {
        "id": "wBIvKdVX-8RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We load the model in CPU again for this section for simplicity\n",
        "# but feel free to keep in GPU if you prefer.\n",
        "# Note: you need to move all the data to the device as well.\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")"
      ],
      "metadata": {
        "id": "b80A-3H_-8RD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\\\n",
        "Translate English to Spanish:\n",
        "\n",
        "English: I do not speak Spanish.\n",
        "Spanish: No hablo español.\n",
        "\n",
        "English: See you later!\n",
        "Spanish: ¡Hasta luego!\n",
        "\n",
        "English: Where is a good restaurant?\n",
        "Spanish: ¿Dónde hay un buen restaurante?\n",
        "\n",
        "English: What rooms do you have available?\n",
        "Spanish: ¿Qué habitaciones tiene disponibles?\n",
        "\n",
        "English: I like soccer\n",
        "Spanish:\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "output = model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=10,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "wvPxVSwP-8RD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Models Genealogy"
      ],
      "metadata": {
        "id": "nxvgVTzd-8RD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder-only Models"
      ],
      "metadata": {
        "id": "CgPpdIBn-8RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_masker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "fill_masker(\"The [MASK] is made of milk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1m1-SiA-8RE",
        "outputId": "224a6702-8682-4ae8-a4be-a46457b6d3f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.19546644389629364,\n",
              "  'token': 9841,\n",
              "  'token_str': 'dish',\n",
              "  'sequence': 'the dish is made of milk.'},\n",
              " {'score': 0.12907548248767853,\n",
              "  'token': 8808,\n",
              "  'token_str': 'cheese',\n",
              "  'sequence': 'the cheese is made of milk.'},\n",
              " {'score': 0.10590681433677673,\n",
              "  'token': 6501,\n",
              "  'token_str': 'milk',\n",
              "  'sequence': 'the milk is made of milk.'},\n",
              " {'score': 0.04112086072564125,\n",
              "  'token': 4392,\n",
              "  'token_str': 'drink',\n",
              "  'sequence': 'the drink is made of milk.'},\n",
              " {'score': 0.0371234267950058,\n",
              "  'token': 7852,\n",
              "  'token_str': 'bread',\n",
              "  'sequence': 'the bread is made of milk.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Power of Pre-training"
      ],
      "metadata": {
        "id": "L_Nkx1A0-8RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The key Insights of Transformers\n"
      ],
      "metadata": {
        "id": "98v-S9gH-8RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        ")\n",
        "classifier(\"This movie is disgustingly good !\")"
      ],
      "metadata": {
        "id": "e-OjQdz2-8RE",
        "outputId": "4ed08a0f-17b5-46eb-a539-613c3a741422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998536109924316}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations\n"
      ],
      "metadata": {
        "id": "TF0f4C5d-8RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK] during summer.\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK] during summer.\")\n",
        "print([r[\"token_str\"] for r in result])"
      ],
      "metadata": {
        "id": "Y7ZkCbcZ-8RF",
        "outputId": "627f13f9-12cf-4da9-ab4f-de3061bb56b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['farmer', 'carpenter', 'gardener', 'fisherman', 'miner']\n",
            "['maid', 'nurse', 'servant', 'waitress', 'cook']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond Text"
      ],
      "metadata": {
        "id": "JKbjiBkR-8RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "from genaibook.core import SampleURL\n",
        "\n",
        "# Download an image and load it with the PIL library\n",
        "url = SampleURL.CatExample\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "FQDFEDMV-8RG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"zero-shot-image-classification\", model=\"openai/clip-vit-base-patch32\"\n",
        ")\n",
        "labels = [\"cat\", \"dog\", \"zebra\"]\n",
        "pipe(image, candidate_labels=labels)\n"
      ],
      "metadata": {
        "id": "RjH0Aq7L-8RG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "A big part of learning is putting your knowledge into practice. We strongly suggest not looking at the answers before taking a serious stab at it. Scroll down for the answers."
      ],
      "metadata": {
        "id": "I1YWS-fV-8RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Time: Using LMs to Generate Text\n",
        "\n",
        "This is the solution for the project. Here we'll implement a `generate` function which supports sampling, `top_k`, and greedy search."
      ],
      "metadata": {
        "id": "iE5Q-xCT-8RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "SDDssYqv-8RH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def generate(\n",
        "    model, tokenizer, input_ids, max_length=50, do_sample=False, top_k=0\n",
        "):\n",
        "    \"\"\"Generate a sequence that starts with `input_ids` without using model.generate().\n",
        "\n",
        "    Args:\n",
        "        model (transformers.PreTrainedModel): The model to use for generation.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use for generation.\n",
        "        input_ids (torch.Tensor): The input IDs\n",
        "        max_length (int, optional): The maximum length of the sequence. Defaults to 50.\n",
        "        do_sample (bool, optional): Whether to use sampling. Defaults to False.\n",
        "        top_k (int, optional): The number of tokens to sample from. Defaults to 0.\n",
        "    \"\"\"\n",
        "    current_length = input_ids.size(1)  # Current sequence length\n",
        "\n",
        "    for _ in range(max_length - current_length):\n",
        "        # Pass the current sequence through the model\n",
        "        outputs = model(input_ids)\n",
        "\n",
        "        # Get logits for the last token in the output\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        if do_sample:\n",
        "            if top_k > 0:\n",
        "                # Apply Top-K filtering\n",
        "                # Get the indices of the top_k logits\n",
        "                top_k_logits, _ = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "                # Get the smallest token from the top_k logits\n",
        "                min_top_k_value = top_k_logits[:, -1].unsqueeze(-1)\n",
        "\n",
        "                # Set values smaller than the smallest top_k value to -infinity (probability 0)\n",
        "                next_token_logits = torch.where(\n",
        "                    next_token_logits < min_top_k_value,\n",
        "                    torch.tensor(float(\"-inf\")),\n",
        "                    next_token_logits,\n",
        "                )\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities for sampling\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            # Sample the next token from the distribution probability\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            # Greedy decoding: choose the token with highest probability\n",
        "            next_token_id = torch.argmax(\n",
        "                next_token_logits, dim=-1, keepdim=True\n",
        "            )\n",
        "\n",
        "        # Append predicted token to the input sequence\n",
        "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
        "\n",
        "        # If the end of sequence token is generated, stop\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "    return tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "pM6EPwrR-8RI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\"It was a dark and stormy\", return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "76Y1R4IU-8RI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first try greedy decoding. As it's deterministic, this should give you the same content as in Chapter 2, which was\n",
        "\n",
        "> It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the"
      ],
      "metadata": {
        "id": "tMkLLHvU-8RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(gpt2, tokenizer, input_ids, do_sample=False)"
      ],
      "metadata": {
        "id": "BVLQry4k-8RI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now try sampling with `top_k`"
      ],
      "metadata": {
        "id": "vA1oaIi9-8RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(gpt2, tokenizer, input_ids, do_sample=True, top_k=10)"
      ],
      "metadata": {
        "id": "pGksPEiD-8RI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "\n",
        "**1. What's the role of the attention mechanism in text generation?**\n",
        "\n",
        "The attention mechanism in text generation allows the model to focus on different parts of the input sequence when producing each token in the output sequence. It helps the model capture long-range dependencies and relationships between words.\n",
        "\n",
        "\n",
        "**2. In which cases would a character-based tokenizer be preferred?**\n",
        "\n",
        "A character-based tokenizer might be preferred in cases where the language involves specific characters, symbols, or emojis that carry meaning. For example, in Chinese, each character carries more information than a character in a Latin language.\n",
        "\n",
        "\n",
        "**3. What happens if you use a tokenizer different from the one used with the model?**\n",
        "\n",
        "Using a different tokenizer can cause a tokenizer mismatch, leading to inconsistencies in tokenization. This misalignment can result in unexpected or incorrect model outputs.\n",
        "\n",
        "**4. What's the risk of using `no_repeat_ngram_size` when doing generation? (Hint: Think of city names.)**\n",
        "\n",
        "If you use a `no_repeat_ngram_size=3`, the \"New York City\" string will only be generated once. If you're using a generative model to generate a story in NYC, using this penalty will severily hurt it!\n",
        "\n",
        "**5. What would happen if you combine Beam-search and sampling?**\n",
        "\n",
        "With beam search, we maintain the `k` best sequences at each timestep. Instead of determinisitcally choosing the top `k` sequences based on the score, we use a probabilistic element and sample the next token from the probability distribution.\n",
        "\n",
        "This can lead to increased diversity (no repetitive outputs of Beam-search), improved quality (no unpredictable outputs of sampling), and a balance between the two. However, it can require more computational resources due to the increased number of sequences to maintain and sample from.\n",
        "\n",
        "**6. Imagine you're using a LLM that generates code in a code editor by doing sampling. What would be more convenient? A low temperature or a high temperature?**\n",
        "\n",
        "In code generation, a low temperature is more convenient as it generates more deterministic and conservative code, adhering to common patterns and reducing the risk of errors. A high temperature would introduce more randomness and could result in unconventional or less reliable code. Therefore, a low temperature is typically preferred.\n",
        "\n",
        "\n",
        "**7. What’s the importance of fine-tuning, and why is it different from zero-shot generation?**\n",
        "\n",
        "Fine-tuning is crucial for adapting a pre-trained language model to specific tasks or domains. It allows the model to learn task-specific nuances and improves performance on targeted applications. This differs from zero-shot generation, where the model is used without task-specific training.\n",
        "\n",
        "\n",
        "**8. Explain the differences and applications of encoder, decoder, and encoderdecoder transformers.**\n",
        "\n",
        "* **Encoder**: Specialized in processing input sequences and extracting meaningful representations. It's commonly used for tasks like classification, sentiment analysis, and document embeddings. Examples include BERT and RoBERTa.\n",
        "* **Decoder**: Specialized in generating output sequences based on given input. It's often used for text generation tasks. Examples include GPT-2 and T5.\n",
        "* **Encoder-Decoder**: Combines both an encoder and a decoder. The encoder processes the input sequence into a set of representations, which the decoder then uses to generate an output sequence. It's ideal for sequence-to-sequence tasks such as machine translation, text summarization, and question answering. Examples include BART and T5."
      ],
      "metadata": {
        "id": "CYCD__yr-8RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenges\n",
        "\n",
        "**9. Summarization. Use a summarization model (you can do `pipeline(\"summarization)`) to generate summaries of a paragraph. How does it compare with the results of using zero-shot? Can it be beaten by providing few-shot examples?**"
      ],
      "metadata": {
        "id": "HrXim4zS-8RJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first solve this using a summarization model. We'll use a default model, but note that it's a small model from 2020. The results might not be as good as the latest or larger models."
      ],
      "metadata": {
        "id": "34JYXL38-8RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")"
      ],
      "metadata": {
        "id": "eONrAHGJ-8RJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pc-5acj2-8RJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer(long_text)"
      ],
      "metadata": {
        "id": "fN0hc5jZ-8RJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now try with a text generation model."
      ],
      "metadata": {
        "id": "ZYBSE_su-8RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")"
      ],
      "metadata": {
        "id": "NFMmnalW-8RK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_with_zero_shot(long_text):\n",
        "    zero_shot = generator(\n",
        "        f\"{long_text}.\\\\n Summary:\\\\n\",\n",
        "        return_full_text=False,\n",
        "        max_length=350,\n",
        "    )\n",
        "\n",
        "    return zero_shot"
      ],
      "metadata": {
        "id": "-HSMB1JS-8RK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_with_zero_shot(long_text)"
      ],
      "metadata": {
        "id": "lBd4gzrN-8RK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you achieve better results by improving the prompt or adding some examples to it?"
      ],
      "metadata": {
        "id": "dJrzh7Wf-8RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Sentiment Analysis. In the zero-shot supplementary material, we calculate some metrics using zero-shot classification. Explore using the `distilbert-base-uncased-finetuned-sst-2-english` encoder model that can do sentiment analysis. What results do you get?**\n",
        "\n",
        "Let's first recall how to use this model. We can either use the `pipeline(\"text-classification\")` approach, or load the model directly with `AutoModel`, which would require handling the tokenization ourselves. We'll want to use `truncation` just as in the zero-shot example above. We begin loading the model and tokenizer. Note that we use `AutoModelForSequenceClassification` to make sure to load the classification layer as well."
      ],
      "metadata": {
        "id": "LYfPQpUX-8RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "fy5C2RdR-8RK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the output with a specific example."
      ],
      "metadata": {
        "id": "VEowpN4W-8RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\n",
        "    \"This movie was terrible!\", truncation=True, return_tensors=\"pt\"\n",
        ").input_ids\n",
        "classifier_output = model(input_ids)\n",
        "classifier_output"
      ],
      "metadata": {
        "id": "3EtWxVRK-8RK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is two logits, which tell us that the first class is more likely than the second. Programmatically, we can do an `argmax` over the `logits` tensor to get the index of the highest value."
      ],
      "metadata": {
        "id": "ozI9gjbL-8RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class_id = classifier_output.logits.argmax().item()\n",
        "predicted_class_id"
      ],
      "metadata": {
        "id": "wnOrzZO_-8RL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To which class does the 0 correspond to? Using the model configuration `id2label`, we can obtain the corresponding class"
      ],
      "metadata": {
        "id": "6r-vnbUt-8RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "EM7cw5Rs-8RL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And using the model configuration, we can get which label correspond to that id."
      ],
      "metadata": {
        "id": "jSI96Qvi-8RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "J_GmuCUm-8RL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's wrap this in a function"
      ],
      "metadata": {
        "id": "-MxQndKI-8RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_with_classifier(sample):\n",
        "    \"\"\"Given a review, predict whether it is positive or negative\"\"\"\n",
        "    input_ids = tokenizer(\n",
        "        sample[\"text\"], truncation=True, return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "    classifier_output = model(input_ids)\n",
        "    sample[\"pred\"] = classifier_output.logits.argmax().item()\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "3YHH-yta-8RM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "score_with_classifier({\"text\": \"This movie was terrible!\"})"
      ],
      "metadata": {
        "id": "7DGtfk54-8RM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "updated_dataset = small_dataset.map(score_with_classifier)\n",
        "updated_dataset"
      ],
      "metadata": {
        "id": "UKNO7wNt-8RM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix.compute(\n",
        "    references=updated_dataset[\"label\"], predictions=updated_dataset[\"pred\"]\n",
        ")\n",
        "cm"
      ],
      "metadata": {
        "id": "D23AuCq--8RM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm[\"confusion_matrix\"],\n",
        "    display_labels=[\"negative\", \"positive\"],\n",
        ")\n",
        "disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "\n",
        "plt.title(\"Normalized confusion matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ict7CyHI-8RM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impressive! This sentiment analysis classifier performs much better than the zero-shot setup, specially for positive reviews which were being misclassified as negative."
      ],
      "metadata": {
        "id": "hDf4HPw3-8RM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Semantic Search. Let's build a FAQ system! ! Sentence transformers are powerful models that can measure semantic text similarity. While the transformer encoder usually outputs an embedding for each token, sentence transformers output an embedding for the whole input text, allowing us to determine if two texts have similar meanings based on their similarity score. Let’s look at a simple example using the `sentence_transformers` library.**"
      ],
      "metadata": {
        "id": "qpXQcGda-8RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the example provided in the book. We first load the `all-MiniLM-L6-v2` model from the Hub. We then use it to compute the embedding of two sentences using the `encode` method. Each is a tensor (vector) of 384 values. We finally compute the cosine similarity to determine how close both tensors are."
      ],
      "metadata": {
        "id": "pkIsfaHU-8RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "sentences = [\"I'm happy\", \"I'm full of happiness\"]\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute embedding for both lists\n",
        "embedding_1 = model.encode(sentences[0], convert_to_tensor=True)\n",
        "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
        "\n",
        "util.pytorch_cos_sim(embedding_1, embedding_2)"
      ],
      "metadata": {
        "id": "L4DtdkKn-8RN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll write a dictionary of 5 questions and answers based on Medicare FAQ website."
      ],
      "metadata": {
        "id": "1xXXFEh6-8RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data from https://faq.ssa.gov/en-US/topic/?id=CAT-01092\n",
        "\n",
        "faq = {\n",
        "    \"How do I get a replacement Medicare card?\": \"If your Medicare card was lost, stolen, or destroyed, you can request a replacement online at Medicare.gov.\",\n",
        "    \"How do I sign up for Medicare?\": \"If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible.\",\n",
        "    \"What are Medicare late enrollment penalties?\": \"In most cases, if you don’t sign up for Medicare when you’re first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995\",\n",
        "    \"Will my Medicare premiums be higher because of my higher income?\": \"Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount.\",\n",
        "    \"What is Medicare and who can get it?\": \"Medicare is a health insurance program for people age 65 or older. Some younger people are eligible for Medicare including people with disabilities, permanent kidney failure and amyotrophic lateral sclerosis (Lou Gehrig’s disease or ALS). Medicare helps with the cost of health care, but it does not cover all medical expenses or the cost of most long-term care.\",\n",
        "}"
      ],
      "metadata": {
        "id": "z-QiA4gm-8RN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `encode` method to compute the embedding of the 5 questions."
      ],
      "metadata": {
        "id": "CaNCW0oq-8RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = model.encode(list(faq.values()), convert_to_tensor=True)\n",
        "print(corpus_embeddings.shape)"
      ],
      "metadata": {
        "id": "ZVGqLI8m-8RN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in a production setup, a user would input a question. We can compute its embedding as well."
      ],
      "metadata": {
        "id": "o4HAnMFl-8RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Do I need to pay more after a raise?\"\n",
        "query_embedding = model.encode(user_question, convert_to_tensor=True)\n",
        "query_embedding.shape"
      ],
      "metadata": {
        "id": "0nIi_HAV-8RO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, using cosine similarity, we can retrieve the most similar questions from the database - that is, we retrieve the most similar questions based on the highest similarity.\n",
        "\n",
        "Using `argsort`, we get the top 3 questions and print them."
      ],
      "metadata": {
        "id": "szvTJFiW-8RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "similarities = -util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "top_3 = similarities.cpu().argsort()[:3]\n",
        "for i, top_n in enumerate(top_3):\n",
        "    print(\n",
        "        f\"Top {i+1} question (p={-similarities[top_n]}): {list(faq.keys())[top_n]}\"\n",
        "    )\n",
        "    print(f\"Answer: {list(faq.values())[top_n]}\")"
      ],
      "metadata": {
        "id": "Z-EpYN3h-8RO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sentence_transformers` also offers a convenient utility called `semantic_search` for exactly this use case."
      ],
      "metadata": {
        "id": "Ua8B277W-8RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = util.semantic_search(\n",
        "    query_embedding, corpus_embeddings, top_k=3\n",
        ")[0]\n",
        "for i, result in enumerate(similarities):\n",
        "    corpus_id = result[\"corpus_id\"]\n",
        "    score = result[\"score\"]\n",
        "    print(f\"Top {i+1} question (p={score}): {list(faq.keys())[corpus_id]}\")\n",
        "    print(f\"Answer: {list(faq.values())[corpus_id]}\")"
      ],
      "metadata": {
        "id": "YgUYMFi0-8RO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}