{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":""},"colab":{"include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/NLP_05_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"markdown","source":"[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/simonmellergaard/nlp-05-text-generation/edit)","metadata":{}},{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n# !git clone https://github.com/nlp-with-transformers/notebooks.git\n# %cd notebooks\n# from install import *\n# install_requirements()","metadata":{"id":"PhQWHBu293O5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\nfrom utils import *\nsetup_chapter()","metadata":{"id":"0_xaDGep93O5","outputId":"42657601-c1cf-4175-9366-b514278c8dd0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Generation","metadata":{"id":"dIWIyVfj93O6"}},{"cell_type":"markdown","source":"<img alt=\"LM Meta Learning\" width=\"800\" caption=\"During pretraining, language models are exposed to sequences of tasks that can be adapted during inference (courtesy of Tom B. Brown)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter05_lm-meta-learning.png?raw=1\" id=\"lm-meta-learning\"/>","metadata":{"id":"TxkVLd5M93O7"}},{"cell_type":"markdown","source":"<img alt=\"Meena\" width=\"300\" caption=\"Meena on the left telling a corny joke to a human on the right (courtesy of Daniel Adiwardana and Thang Luong)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter05_meena.png?raw=1\" id=\"meena\"/>","metadata":{"id":"SKEekktb93O8"}},{"cell_type":"markdown","source":"## The Challenge with Generating Coherent Text","metadata":{"id":"AWFvFoSu93O8"}},{"cell_type":"markdown","source":"<img alt=\"Text generation\" width=\"700\" caption=\"Generating text from an input sequence by adding a new word to the input at each step\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter05_text-generation.png?raw=1\" id=\"text-generation\"/>","metadata":{"id":"ieazl0JP93O9"}},{"cell_type":"markdown","source":"## Greedy Search Decoding","metadata":{"id":"uqZx2CPI93O9"}},{"cell_type":"code","source":"# hide_output\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"id":"qMtcsPYC93O-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide_output\nimport pandas as pd\n\ninput_txt = \"Transformers are the\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\niterations = []\nn_steps = 8\nchoices_per_step = 5\n\nwith torch.no_grad():\n    for _ in range(n_steps):\n        iteration = dict()\n        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n        output = model(input_ids=input_ids)\n        # Select logits of the first batch and the last token and apply softmax\n        next_token_logits = output.logits[0, -1, :]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n        # Store tokens with highest probabilities\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy()\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        # Append predicted next token to input\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n        iterations.append(iteration)\n\npd.DataFrame(iterations)","metadata":{"id":"WJxRjgh493O_","outputId":"b079a0b2-730f-4a88-b49a-c73e772cb634"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(tokenizer.decode(output[0]))","metadata":{"id":"yzs6PDcC93O_","outputId":"f243eb7e-c1d6-43ff-9ea4-e7e112a5df16"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length,\n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))","metadata":{"id":"ZEqQcqVg93O_","outputId":"77943b68-3855-435a-b62e-96800adcf75a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Beam Search Decoding","metadata":{"id":"I8JwAG1o93PA"}},{"cell_type":"markdown","source":"<img alt=\"Beam search\" width=\"700\" caption=\"Beam search with two beamsâ€”the most probable sequences at each timestep are highlighted in blue\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter05_beam-search.png?raw=1\" id=\"beam-search\"/>","metadata":{"id":"EGdkBeq493PA"}},{"cell_type":"code","source":"0.5 ** 1024","metadata":{"id":"MaWDGYw-93PA","outputId":"e1b3d5a8-705a-4db7-d5c1-264c4a37137e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nsum([np.log(0.5)] * 1024)","metadata":{"id":"apRTrxzb93PA","outputId":"427a46a4-6bad-447d-d04d-edb548db3b35"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim=-1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    return logp_label","metadata":{"id":"kc5XG89c93PB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sequence_logprob(model, labels, input_len=0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(\n            output.logits[:, :-1, :], labels[:, 1:])\n        seq_log_prob = torch.sum(log_probs[:, input_len:])\n    return seq_log_prob.cpu().numpy()","metadata":{"id":"bAgAsvYM93PB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_greedy[0]))\nprint(f\"\\nlog-prob: {logp:.2f}\")","metadata":{"id":"qBQDjKsS93PC","outputId":"8c675f42-33c6-44d1-f071-5094ec3aeff2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n                             do_sample=False)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\nlog-prob: {logp:.2f}\")","metadata":{"id":"A3ko71y693PC","outputId":"1859b858-5ff6-4a6a-daba-d095dcf92609"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n                             do_sample=False, no_repeat_ngram_size=2)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\nlog-prob: {logp:.2f}\")","metadata":{"id":"DUNfKYUt93PC","outputId":"eca5b1ed-413e-4907-81f0-cd267b60d625"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sampling Methods","metadata":{"id":"fJI8wgOW93PC"}},{"cell_type":"code","source":"#hide_input\n\n#id temperature\n#alt Token probabilities as a function of temperature\n#caption Distribution of randomly generated token probabilities for three selected temperatures\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax(logits, T=1):\n    e_x = np.exp(logits / T)\n    return e_x / e_x.sum()\n\nlogits = np.exp(np.random.random(1000))\nsorted_logits = np.sort(logits)[::-1]\nx = np.arange(1000)\n\nfor T in [0.5, 1.0, 2.0]:\n    plt.step(x, softmax(sorted_logits, T), label=f\"T={T}\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Sorted token probabilities\")\nplt.ylabel(\"Probability\")\nplt.show()","metadata":{"id":"v18BPBOE93PC","outputId":"968fb2b7-db4f-4478-8854-5940b3606838"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\ntorch.manual_seed(42);","metadata":{"id":"bolZX1nC93PD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=2.0, top_k=0)\nprint(tokenizer.decode(output_temp[0]))","metadata":{"id":"6vk_DXiL93PD","outputId":"5964c7b4-723f-4ad2-94b1-f3a7c726c6f1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\ntorch.manual_seed(42);","metadata":{"id":"9JSZJyoH93PD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=0.5, top_k=0)\nprint(tokenizer.decode(output_temp[0]))","metadata":{"id":"q80fdNoJ93PD","outputId":"c2879a59-17f0-405b-e576-b8bb49cad1b5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Top-k and Nucleus Sampling","metadata":{"id":"Ntgqabz093PD"}},{"cell_type":"code","source":"# hide\ntorch.manual_seed(42);","metadata":{"id":"1fmj_Wlm93PD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)","metadata":{"id":"barNp9EP93PD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\nimport torch.nn.functional as F\n\nwith torch.no_grad():\n    output = model(input_ids=input_ids)\n    next_token_logits = output.logits[:, -1, :]\n    probs = F.softmax(next_token_logits, dim=-1).detach().cpu().numpy()","metadata":{"id":"_yA8WXrm93PE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide_input\n\n#id distribution\n#alt Probability distribution of next token prediction.\n#caption Probability distribution of next token prediction (left) and cumulative distribution of descending token probabilities\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n\naxes[0].hist(probs[0], bins=np.logspace(-10, -1, 100), color=\"C0\", edgecolor=\"C0\")\naxes[0].set_xscale(\"log\")\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Probability distribution\")\naxes[0].set_xlabel(\"Probability\")\naxes[0].set_ylabel(\"Count\")\n#axes[0].grid(which=\"major\")\n\naxes[1].plot(np.cumsum(np.sort(probs[0])[::-1]), color=\"black\")\naxes[1].set_xlim([0, 10000])\naxes[1].set_ylim([0.75, 1.01])\naxes[1].set_title(\"Cumulative probability\")\naxes[1].set_ylabel(\"Probability\")\naxes[1].set_xlabel(\"Token (descending probability)\")\n#axes[1].grid(which=\"major\")\naxes[1].minorticks_on()\n#axes[1].grid(which='minor', linewidth='0.5')\ntop_k_label = 'top-k threshold (k=2000)'\ntop_p_label = 'nucleus threshold (p=0.95)'\naxes[1].vlines(x=2000, ymin=0, ymax=2, color='C0', label=top_k_label)\naxes[1].hlines(y=0.95, xmin=0, xmax=10000, color='C1', label=top_p_label, linestyle='--')\naxes[1].legend(loc='lower right')\nplt.tight_layout()","metadata":{"id":"0rDfurGp93PE","outputId":"802115cd-bcf4-4f96-e5b1-94b99daab334"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\ntorch.manual_seed(42);","metadata":{"id":"wcbtY-Qi93PE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             top_k=50)\nprint(tokenizer.decode(output_topk[0]))","metadata":{"id":"wcWRRcVs93PE","outputId":"81c11c33-68a9-494a-eb22-a9980d37602a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\ntorch.manual_seed(42);","metadata":{"id":"8wtjqPRv93PF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             top_p=0.90)\nprint(tokenizer.decode(output_topp[0]))","metadata":{"id":"ll12_ORn93PF","outputId":"e065b35f-10b1-445b-a42d-49ebaa1f0bae"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Which Decoding Method Is Best?","metadata":{"id":"TPz2ycyA93PF"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"jsLZ-4o293PF"}}]}