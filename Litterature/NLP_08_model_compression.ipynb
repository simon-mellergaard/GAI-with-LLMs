{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/NLP_08_model_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"markdown","source":"[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/simonmellergaard/nlp-08-model-compression/edit)","metadata":{}},{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n# !git clone https://github.com/nlp-with-transformers/notebooks.git\n# %cd notebooks\n# from install import *\n# install_requirements()","metadata":{"id":"gmrSz3Cs-TSn"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hide\nfrom utils import *\nsetup_chapter()","metadata":{"id":"ihNxoKmy-TSo","outputId":"f02b5747-f9bb-4271-f744-1525e2f92d64"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making Transformers Efficient in Production","metadata":{"id":"fee5oYPb-TSp"}},{"cell_type":"markdown","source":"<img alt=\"Scaling BERT at Roblox\" caption=\"How Roblox scaled BERT with knowledge distillation, dynamic padding, and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip Kaehler)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_roblox.png?raw=1\" id=\"roblox\"/>","metadata":{"id":"P9tXBzWz-TSq"}},{"cell_type":"markdown","source":"## Intent Detection as a Case Study","metadata":{"id":"FbpvJ_dg-TSr"}},{"cell_type":"markdown","source":"<img alt=\"Out of Scope Query\" width=\"400\" caption=\"Three exchanges between a human (right) and a text-based assistant (left) for personal finance (courtesy of Stefan Larson et al.)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_oos.png?raw=1\" id=\"oos\"/>","metadata":{"id":"2Nr7YJz0-TSr"}},{"cell_type":"code","source":"#hide_output\nfrom transformers import pipeline\n\nbert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=bert_ckpt)","metadata":{"colab":{"referenced_widgets":["9901d62b758e4ac0a56491c7d3533e2e","0b5acf4953f64336ae51a8d24fd9b474","877733b160d44893a2153916804e3a94","f53e5fee96484d75ad627b02e4881a5f","665ec4fe2a38444fb1ca883481a50208"]},"id":"ENOzILTA-TSs","outputId":"ee5fe5d0-04cb-4a27-9031-f9db8e20b2ee"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in\nParis and I need a 15 passenger van\"\"\"\npipe(query)","metadata":{"id":"uuO7aVHv-TSt","outputId":"a2a6ca21-4bc5-40db-be16-27c601cb86c1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a Performance Benchmark","metadata":{"id":"iYFP4Eeh-TSt"}},{"cell_type":"code","source":"class PerformanceBenchmark:\n    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n        self.pipeline = pipeline\n        self.dataset = dataset\n        self.optim_type = optim_type\n\n    def compute_accuracy(self):\n        # We'll define this later\n        pass\n\n    def compute_size(self):\n        # We'll define this later\n        pass\n\n    def time_pipeline(self):\n        # We'll define this later\n        pass\n\n    def run_benchmark(self):\n        metrics = {}\n        metrics[self.optim_type] = self.compute_size()\n        metrics[self.optim_type].update(self.time_pipeline())\n        metrics[self.optim_type].update(self.compute_accuracy())\n        return metrics","metadata":{"id":"QMb_lP6m-TSt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nfrom datasets import load_dataset\n\nclinc = load_dataset(\"clinc_oos\", \"plus\")","metadata":{"colab":{"referenced_widgets":["83c7ad636fb142a98226c974a6248fe9","0b7121ed5b5640f29a9a8437631068d0","331e4a742c864e5496b10fd4a0588d86","","607e6702bf3043b1bfde3e8eb760021f"]},"id":"2zRLbOUc-TSu","outputId":"f62ce007-c978-46d3-acbf-b45299b618b8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = clinc[\"test\"][42]\nsample","metadata":{"id":"LuajUiIu-TSu","outputId":"04844112-ca3b-43a8-a9a9-e29f9179cf6c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"intents = clinc[\"test\"].features[\"intent\"]\nintents.int2str(sample[\"intent\"])","metadata":{"id":"wUOGWotM-TSu","outputId":"45c700b3-fbda-4dbb-e4be-53871a88e198"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nfrom datasets import load_metric\n\naccuracy_score = load_metric(\"accuracy\")","metadata":{"colab":{"referenced_widgets":["e24b9b0253964af0b0a891e4ea8859d0"]},"id":"LybR2NAR-TSv","outputId":"245798f4-d5ab-4b31-a127-7d344fd2aae5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_accuracy(self):\n    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n    preds, labels = [], []\n    for example in self.dataset:\n        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n        label = example[\"intent\"]\n        preds.append(intents.str2int(pred))\n        labels.append(label)\n    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n    return accuracy\n\nPerformanceBenchmark.compute_accuracy = compute_accuracy","metadata":{"id":"lcNRfZaW-TSv"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(pipe.model.state_dict().items())[42]","metadata":{"id":"_pztubRh-TSv","outputId":"e14b4835-f49e-42d7-e25c-474d93683386"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom pathlib import Path\n\ndef compute_size(self):\n    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n    state_dict = self.pipeline.model.state_dict()\n    tmp_path = Path(\"model.pt\")\n    torch.save(state_dict, tmp_path)\n    # Calculate size in megabytes\n    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n    # Delete temporary file\n    tmp_path.unlink()\n    print(f\"Model size (MB) - {size_mb:.2f}\")\n    return {\"size_mb\": size_mb}\n\nPerformanceBenchmark.compute_size = compute_size","metadata":{"id":"vNgW2c0J-TSv"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from time import perf_counter\n\nfor _ in range(3):\n    start_time = perf_counter()\n    _ = pipe(query)\n    latency = perf_counter() - start_time\n    print(f\"Latency (ms) - {1000 * latency:.3f}\")","metadata":{"id":"RrMz807D-TSw","outputId":"1424c21e-ba19-43ee-c6ad-91ce88412545"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef time_pipeline(self, query=\"What is the pin number for my account?\"):\n    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n    latencies = []\n    # Warmup\n    for _ in range(10):\n        _ = self.pipeline(query)\n    # Timed run\n    for _ in range(100):\n        start_time = perf_counter()\n        _ = self.pipeline(query)\n        latency = perf_counter() - start_time\n        latencies.append(latency)\n    # Compute run statistics\n    time_avg_ms = 1000 * np.mean(latencies)\n    time_std_ms = 1000 * np.std(latencies)\n    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n\nPerformanceBenchmark.time_pipeline = time_pipeline","metadata":{"id":"wE9xh3dy-TSw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pb = PerformanceBenchmark(pipe, clinc[\"test\"])\nperf_metrics = pb.run_benchmark()","metadata":{"id":"mqSWF3CE-TSw","outputId":"147c296d-ac91-4316-d727-9e56fa9af437"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making Models Smaller via Knowledge Distillation","metadata":{"id":"XoN1YSyT-TSw"}},{"cell_type":"markdown","source":"### Knowledge Distillation for Fine-Tuning","metadata":{"id":"ZGG9MR8m-TSx"}},{"cell_type":"markdown","source":"<img alt=\"Soft Probabilities\" caption=\"Comparison of a hard label that is one-hot encoded (left), softmax probabilities (middle), and softened class probabilities (right)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_soft-probs.png?raw=1\" id=\"soft-probs\"/>","metadata":{"id":"J-9vjpNw-TSx"}},{"cell_type":"markdown","source":"<img alt=\"Knowledge distillation\" caption=\"The knowledge distillation process\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_kd.png?raw=1\" id=\"kd\"/>","metadata":{"id":"M7eOYDyi-TSx"}},{"cell_type":"markdown","source":"### Knowledge Distillation for Pretraining","metadata":{"id":"2CP_rX9a-TSx"}},{"cell_type":"markdown","source":"### Creating a Knowledge Distillation Trainer","metadata":{"id":"2kwcFo45-TSx"}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.temperature = temperature","metadata":{"id":"wTEumOly-TSx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        inputs = inputs.to(device)\n        outputs_stu = model(**inputs)\n        # Extract cross-entropy loss and logits from student\n        loss_ce = outputs_stu.loss\n        logits_stu = outputs_stu.logits\n        # Extract logits from teacher\n        with torch.no_grad():\n            outputs_tea = self.teacher_model(**inputs)\n            logits_tea = outputs_tea.logits\n        # Soften probabilities and compute distillation loss\n        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n        loss_kd = self.args.temperature ** 2 * loss_fct(\n            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n            F.softmax(logits_tea / self.args.temperature, dim=-1))\n        # Return weighted student loss\n        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n        return (loss, outputs_stu) if return_outputs else loss","metadata":{"id":"8mY5Nr8C-TSx"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Choosing a Good Student Initialization","metadata":{"id":"4y4s5p_S-TSx"}},{"cell_type":"code","source":"#hide_output\nfrom transformers import AutoTokenizer\n\nstudent_ckpt = \"distilbert-base-uncased\"\nstudent_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n\ndef tokenize_text(batch):\n    return student_tokenizer(batch[\"text\"], truncation=True)\n\nclinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\nclinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")","metadata":{"colab":{"referenced_widgets":["9cbc667acbd944f593b83c21c8889ce8","c35ba476d0e44ccbb15409660bfe1642","a1a17d9f5cb147c59d1da623957ed4d6","50d7ff524239455292ce226c129b9c28","4b3300485702445aba4a28f61e9678bc","19c19d8e651e4ce1ade55f2fd6170c77","4461fcb4d0e743ed84a0814a642ba73b"]},"id":"eM0gBrc1-TSy","outputId":"1bd4306d-5b0e-4608-f13c-16ac09c7f79b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"4tFSkVKR-TSy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(pred):\n    predictions, labels = pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy_score.compute(predictions=predictions, references=labels)","metadata":{"id":"43YdzQ99-TSy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 48\n\nfinetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\nstudent_training_args = DistillationTrainingArguments(\n    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\",\n    num_train_epochs=5, learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\n    push_to_hub=True)","metadata":{"id":"cenXYmqV-TSy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide\nstudent_training_args.logging_steps = len(clinc_enc['train']) // batch_size\nstudent_training_args.disable_tqdm = False\nstudent_training_args.save_steps = 1e9\nstudent_training_args.log_level = 40","metadata":{"id":"npnPW7vO-TSz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide\n%env TOKENIZERS_PARALLELISM=false","metadata":{"id":"jZJCG2f9-TSz","outputId":"17ae285c-e7b1-4a75-a481-55c45dd41233"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"id2label = pipe.model.config.id2label\nlabel2id = pipe.model.config.label2id","metadata":{"id":"lTUb4uNp-TSz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoConfig\n\nnum_labels = intents.num_classes\nstudent_config = (AutoConfig\n                  .from_pretrained(student_ckpt, num_labels=num_labels,\n                                   id2label=id2label, label2id=label2id))","metadata":{"id":"QIZyadBR-TSz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef student_init():\n    return (AutoModelForSequenceClassification\n            .from_pretrained(student_ckpt, config=student_config).to(device))","metadata":{"id":"LiIK8XjY-TSz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nteacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\nteacher_model = (AutoModelForSequenceClassification\n                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\n                 .to(device))","metadata":{"id":"p0tJ12lf-TS0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"distilbert_trainer = DistillationTrainer(model_init=student_init,\n    teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n\ndistilbert_trainer.train()","metadata":{"id":"wOMKI6Sx-TS4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\ndistilbert_trainer.push_to_hub(\"Training completed!\")","metadata":{"id":"0ZZbDXdr-TS4","outputId":"955aa145-6709-4c2d-a755-fcc321a079bb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nfinetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=finetuned_ckpt)","metadata":{"id":"OnjGmyRA-TS4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optim_type = \"DistilBERT\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())","metadata":{"id":"ITzB7ixX-TS4","outputId":"0e5c193d-863e-4fce-d95a-307539a40ee7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef plot_metrics(perf_metrics, current_optim_type):\n    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n\n    for idx in df.index:\n        df_opt = df.loc[idx]\n        # Add a dashed circle around the current optimization type\n        if idx == current_optim_type:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\n                        marker='$\\u25CC$')\n        else:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n\n    legend = plt.legend(bbox_to_anchor=(1,1))\n    for handle in legend.legendHandles:\n        handle.set_sizes([20])\n\n    plt.ylim(80,90)\n    # Use the slowest model to define the x-axis range\n    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n    plt.xlim(1, xlim)\n    plt.ylabel(\"Accuracy (%)\")\n    plt.xlabel(\"Average latency (ms)\")\n    plt.show()\n\nplot_metrics(perf_metrics, optim_type)","metadata":{"id":"Po7QDD_3-TS5","outputId":"a59ffd3d-acf2-4574-8e7b-e24575a0ee00"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding Good Hyperparameters with Optuna","metadata":{"id":"JDs3euC0-TS5"}},{"cell_type":"code","source":"#hide_input\n#id banana-function\n#alt A banana plot\n#caption Plot of the Rosenbrock function of two variables\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef f(x, y):\n    return (1-x)**2+100*(y-x**2)**2\n\nX, Y = np.meshgrid(np.linspace(-2, 2, 250), np.linspace(-1, 3, 250))\nZ = f(X,Y)\n_, ax = plt.subplots()\nax.plot([1], [1], 'x', mew=3, markersize=10, color=\"red\")\nax.contourf(X, Y, Z, np.logspace(-1, 3, 30), cmap='viridis', extend=\"both\")\nax.set_xlim(-1.3, 1.3)\nax.set_ylim(-0.9, 1.7)\nplt.show()","metadata":{"id":"uYu7KZw3-TS5","outputId":"27f5fe84-0f08-43ea-e014-4d2661b02c93"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    x = trial.suggest_float(\"x\", -2, 2)\n    y = trial.suggest_float(\"y\", -2, 2)\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2","metadata":{"id":"nMYUQxe_-TS5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nimport optuna\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=1000)","metadata":{"id":"uu2nwqKY-TS5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study.best_params","metadata":{"id":"DFq5Ani--TS6","outputId":"4d7a33c1-10c2-4e63-ec05-34dd57166cb4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hp_space(trial):\n    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}","metadata":{"id":"z6ixp1f1-TS6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nbest_run = distilbert_trainer.hyperparameter_search(\n    n_trials=20, direction=\"maximize\", hp_space=hp_space)","metadata":{"id":"tP78-oYT-TS6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(best_run)","metadata":{"id":"toJ9WOaw-TS6","outputId":"2c19cbfc-a60a-4e8f-bc8f-9962dfadc78d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nfor k,v in best_run.hyperparameters.items():\n    setattr(student_training_args, k, v)\n\n# Define a new repository to store our distilled model\ndistilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\nstudent_training_args.output_dir = distilled_ckpt\n\n# Create a new Trainer with optimal parameters\ndistil_trainer = DistillationTrainer(model_init=student_init,\n    teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n\ndistil_trainer.train();","metadata":{"id":"eDMp6quB-TS6","outputId":"7645a8af-c50d-4b9d-f55d-503659216bb4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\ndistil_trainer.push_to_hub(\"Training complete\")","metadata":{"id":"fTY2u8OM-TS6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Benchmarking Our Distilled Model","metadata":{"id":"XZn7_iF_-TS7"}},{"cell_type":"code","source":"distilled_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\npipe = pipeline(\"text-classification\", model=distilled_ckpt)\noptim_type = \"Distillation\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())","metadata":{"id":"YNXTxI4c-TS7","outputId":"0fb931e3-0c65-40d3-8144-2d2b04237f7a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(perf_metrics, optim_type)","metadata":{"id":"z1u3CV2x-TS7","outputId":"a227c610-4c5b-4edf-e091-d54896ad0618"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making Models Faster with Quantization","metadata":{"id":"whpszY-e-TS7"}},{"cell_type":"markdown","source":"### Sidebar: A Primer on Floating-Point and Fixed-Point Numbers","metadata":{"id":"0kuM9f0O-TS7"}},{"cell_type":"markdown","source":"### End sidebar","metadata":{"id":"gX6_9Oc3-TS7"}},{"cell_type":"markdown","source":"<img alt=\"Mapping floating-point numbers to 8-bit integers\" width=\"800\" caption=\"Quantizing floating-point numbers as unsigned 8-bit integers (courtesy of Manas Sahni)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_fp32-to-int8.png?raw=1\" id=\"fp32toint8\"/>","metadata":{"id":"ct9SLFLa-TS8"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nstate_dict = pipe.model.state_dict()\nweights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\nplt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\nplt.show()","metadata":{"id":"ZV8y_Xj0-TS8","outputId":"670dec21-d685-4a4e-91f1-f02c7cd9fd3c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zero_point = 0\nscale = (weights.max() - weights.min()) / (127 - (-128))","metadata":{"id":"hXtkRAI2-TS8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(weights / scale + zero_point).clamp(-128, 127).round().char()","metadata":{"id":"alFFZWeg-TS8","outputId":"8790ec8b-86c1-476b-d537-37b10c542854"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import quantize_per_tensor\n\ndtype = torch.qint8\nquantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\nquantized_weights.int_repr()","metadata":{"id":"WlZE1JO1-TS8","outputId":"7009b433-8e32-4d9e-d911-53a8a5110a66"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_input\n#id weight-quantization\n#alt Effect of quantization on a transformer's weights\n#caption Effect of quantization on a transformer's weights\nfrom mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n\n# Create histogram\nfig, ax = plt.subplots()\nax.hist(quantized_weights.dequantize().flatten().numpy(),\n         bins=250, range=(-0.3,0.3), edgecolor=\"C0\");\n# Create zoom inset\naxins = zoomed_inset_axes(ax, 5, loc='upper right')\naxins.hist(quantized_weights.dequantize().flatten().numpy(),\n         bins=250, range=(-0.3,0.3));\nx1, x2, y1, y2 = 0.05, 0.1, 500, 2500\naxins.set_xlim(x1, x2)\naxins.set_ylim(y1, y2)\naxins.axes.xaxis.set_visible(False)\naxins.axes.yaxis.set_visible(False)\nmark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\nplt.show()","metadata":{"id":"DGHh3uyW-TS8","outputId":"f50af91e-2172-41d3-d4c2-4bb61a1896b1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%timeit\nweights @ weights","metadata":{"id":"K8gliHIt-TS9","outputId":"ed40c4c7-3532-419b-c6cc-36c62d4b3787"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.quantized import QFunctional\n\nq_fn = QFunctional()","metadata":{"id":"ISX3DP3f-TS9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%timeit\nq_fn.mul(quantized_weights, quantized_weights)","metadata":{"id":"DSWojiyi-TS9","outputId":"4a43fff6-5773-4ff8-a350-e301ef0a6e15"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n\nsys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())","metadata":{"id":"RY-8hSyo-TS9","outputId":"12c1a5b0-0db1-4645-e2bb-d7bca59baeb6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nfrom torch.quantization import quantize_dynamic\n\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt).to(\"cpu\"))\n\nmodel_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)","metadata":{"id":"KiWYPBYo-TS9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Benchmarking Our Quantized Model","metadata":{"id":"AWMM8XX0-TS9"}},{"cell_type":"code","source":"pipe = pipeline(\"text-classification\", model=model_quantized,\n                tokenizer=tokenizer)\noptim_type = \"Distillation + quantization\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())","metadata":{"id":"8XiDZkXU-TS-","outputId":"aa902de5-3caf-46f2-a6a7-f10c866a939f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(perf_metrics, optim_type)","metadata":{"id":"_YGkgcsC-TS-","outputId":"6df94dd5-48bc-42bd-ada4-0d5275a3fcce"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizing Inference with ONNX and the ONNX Runtime","metadata":{"id":"y2MeLszQ-TS-"}},{"cell_type":"markdown","source":"<img alt=\"Example ONNX graph\" width=\"500\" caption=\"A section of the ONNX graph for BERT-base, visualized in Netron\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_bert-onnx.png?raw=1\" id=\"bert-onnx\"/>","metadata":{"id":"c3cor75r-TS-"}},{"cell_type":"markdown","source":"<img alt=\"Architecture of the ONNX and ONNX Runtime ecosystem\" width=\"500\" caption=\"Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of the ONNX Runtime team)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_onnx-ort.png?raw=1\" id=\"onnx-ort\"/>","metadata":{"id":"rvNR7Y2Q-TS-"}},{"cell_type":"code","source":"#hide_output\nimport os\nfrom psutil import cpu_count\n\nos.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\nos.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\"","metadata":{"id":"Ajm6KuTn-TS-","outputId":"5c18179a-cdda-47a8-a647-a8e55b44dd5d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hide_output\nfrom transformers.convert_graph_to_onnx import convert\n\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\nonnx_model_path = Path(\"onnx/model.onnx\")\nconvert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\n        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")","metadata":{"id":"0u4sg4m0-TS-","outputId":"c3ad36e1-7439-40e8-9c0d-fdd1b1a35fed"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from onnxruntime import (GraphOptimizationLevel, InferenceSession,\n                         SessionOptions)\n\ndef create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\n    options = SessionOptions()\n    options.intra_op_num_threads = 1\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n    session = InferenceSession(str(model_path), options, providers=[provider])\n    session.disable_fallback()\n    return session","metadata":{"id":"JMd3SZjg-TS-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onnx_model = create_model_for_provider(onnx_model_path)","metadata":{"id":"0BLO-8wE-TS_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = clinc_enc[\"test\"][:1]\ndel inputs[\"labels\"]\nlogits_onnx = onnx_model.run(None, inputs)[0]\nlogits_onnx.shape","metadata":{"id":"r3KxFk5U-TS_","outputId":"d0ce6496-bd85-4663-cee6-2894af4ed18c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.argmax(logits_onnx)","metadata":{"id":"0KWVSQUT-TS_","outputId":"4a4ed151-5d79-46db-bd16-9e0f819cd1fd"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clinc_enc[\"test\"][0][\"labels\"]","metadata":{"id":"XiZV8Jxd-TS_","outputId":"be41a886-b434-4ed5-ca22-c98e2c0ff87b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.special import softmax\n\nclass OnnxPipeline:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def __call__(self, query):\n        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n        inputs_onnx = {k: v.cpu().detach().numpy()\n                       for k, v in model_inputs.items()}\n        logits = self.model.run(None, inputs_onnx)[0][0, :]\n        probs = softmax(logits)\n        pred_idx = np.argmax(probs).item()\n        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]","metadata":{"id":"uaNjUs99-TS_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = OnnxPipeline(onnx_model, tokenizer)\npipe(query)","metadata":{"id":"GoCUZy0U-TS_","outputId":"5e281155-7a3c-431b-843c-4a930ab8938d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class OnnxPerformanceBenchmark(PerformanceBenchmark):\n    def __init__(self, *args, model_path, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_path = model_path\n\n    def compute_size(self):\n        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n        print(f\"Model size (MB) - {size_mb:.2f}\")\n        return {\"size_mb\": size_mb}","metadata":{"id":"qLOdkyKo-TTA"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optim_type = \"Distillation + ORT\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n                              model_path=\"onnx/model.onnx\")\nperf_metrics.update(pb.run_benchmark())","metadata":{"id":"0jyzfG-a-TTA","outputId":"c7992d5e-e34c-440b-d642-04dcf3d82f4f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(perf_metrics, optim_type)","metadata":{"id":"TLxgbaP8-TTA","outputId":"97ae96a9-b4cf-4c54-df17-f68b112c375d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from onnxruntime.quantization import quantize_dynamic, QuantType\n\nmodel_input = \"onnx/model.onnx\"\nmodel_output = \"onnx/model.quant.onnx\"\nquantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)","metadata":{"id":"T7qE7dxS-TTB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onnx_quantized_model = create_model_for_provider(model_output)\npipe = OnnxPipeline(onnx_quantized_model, tokenizer)\noptim_type = \"Distillation + ORT (quantized)\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n                              model_path=model_output)\nperf_metrics.update(pb.run_benchmark())","metadata":{"id":"AcYpEnYH-TTB","outputId":"efcf0be0-ee6d-44b4-ced7-60849c515f6b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(perf_metrics, optim_type)","metadata":{"id":"59zVHWNe-TTB","outputId":"d64a6fc9-4113-47f4-944e-6637793e695a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making Models Sparser with Weight Pruning","metadata":{"id":"69WeejVJ-TTB"}},{"cell_type":"markdown","source":"### Sparsity in Deep Neural Networks","metadata":{"id":"DjACbL0O-TTB"}},{"cell_type":"markdown","source":"<img alt=\"Network Pruning\" width=\"500\" caption=\"Weights and neurons before and after pruning (courtesy of Song Han)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_network-pruning.png?raw=1\" id=\"network-pruning\"/>","metadata":{"id":"TvQn5oWS-TTC"}},{"cell_type":"markdown","source":"### Weight Pruning Methods","metadata":{"id":"1bRa1-nQ-TTC"}},{"cell_type":"markdown","source":"#### Magnitude pruning","metadata":{"id":"QjEsPvk2-TTC"}},{"cell_type":"code","source":"#hide_input\n#id sparsity-scheduler\n#alt Sparsity scheduler\n#caption The cubic sparsity scheduler used for pruning\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef _sparsity(t, t_0=0, dt=1, s_i=0, s_f=0.9, N=100):\n    return s_f + (s_i - s_f) * (1 - (t - t_0) / (N * dt))**3\n\nsteps = np.linspace(0,100,100)\nvalues = [_sparsity(t) for t in steps]\n\nfig, ax = plt.subplots()\nax.plot(steps, values)\nax.set_ylim(0,1)\nax.set_xlim(0,100)\nax.set_xlabel(\"Pruning step\")\nax.set_ylabel(\"Sparsity\")\nplt.grid(linestyle=\"dashed\")\nplt.show()","metadata":{"id":"Ucf3VydT-TTC","outputId":"f72af85c-b0c5-46f1-c7a6-05a7ac5f667c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Movement pruning","metadata":{"id":"To1IG_Ua-TTC"}},{"cell_type":"markdown","source":"<img alt=\"Magnitude vs Movement Pruning\" width=\"700\" caption=\"Comparison of weights removed (in gray) during magnitude pruning (left) and movement pruning (right)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_magnitude-vs-movement.png?raw=1\" id=\"magnitude-vs-movement\"/>","metadata":{"id":"46DTx2XO-TTD"}},{"cell_type":"markdown","source":"<img alt=\"Pruning Distributions\" width=\"500\" caption=\"Distribution of remaining weights for magnitude pruning (MaP) and movement pruning (MvP)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_pruning-dists.png?raw=1\" id=\"pruning-dists\"/>","metadata":{"id":"c4aa7_wI-TTD"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"yYdFLS0H-TTD"}}]}