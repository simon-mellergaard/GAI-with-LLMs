{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/NLP_08_model_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmrSz3Cs-TSn"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run this cell if you're on Colab or Kaggle\n",
        "# !git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "# %cd notebooks\n",
        "# from install import *\n",
        "# install_requirements()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihNxoKmy-TSo",
        "outputId": "f02b5747-f9bb-4271-f744-1525e2f92d64"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "from utils import *\n",
        "setup_chapter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fee5oYPb-TSp"
      },
      "source": [
        "# Making Transformers Efficient in Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9tXBzWz-TSq"
      },
      "source": [
        "<img alt=\"Scaling BERT at Roblox\" caption=\"How Roblox scaled BERT with knowledge distillation, dynamic padding, and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip Kaehler)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_roblox.png?raw=1\" id=\"roblox\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbpvJ_dg-TSr"
      },
      "source": [
        "## Intent Detection as a Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nr7YJz0-TSr"
      },
      "source": [
        "<img alt=\"Out of Scope Query\" width=\"400\" caption=\"Three exchanges between a human (right) and a text-based assistant (left) for personal finance (courtesy of Stefan Larson et al.)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_oos.png?raw=1\" id=\"oos\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9901d62b758e4ac0a56491c7d3533e2e",
            "0b5acf4953f64336ae51a8d24fd9b474",
            "877733b160d44893a2153916804e3a94",
            "f53e5fee96484d75ad627b02e4881a5f",
            "665ec4fe2a38444fb1ca883481a50208"
          ]
        },
        "id": "ENOzILTA-TSs",
        "outputId": "ee5fe5d0-04cb-4a27-9031-f9db8e20b2ee"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from transformers import pipeline\n",
        "\n",
        "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=bert_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuO7aVHv-TSt",
        "outputId": "a2a6ca21-4bc5-40db-be16-27c601cb86c1"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in\n",
        "Paris and I need a 15 passenger van\"\"\"\n",
        "pipe(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYFP4Eeh-TSt"
      },
      "source": [
        "## Creating a Performance Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMb_lP6m-TSt"
      },
      "outputs": [],
      "source": [
        "class PerformanceBenchmark:\n",
        "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
        "        self.pipeline = pipeline\n",
        "        self.dataset = dataset\n",
        "        self.optim_type = optim_type\n",
        "\n",
        "    def compute_accuracy(self):\n",
        "        # We'll define this later\n",
        "        pass\n",
        "\n",
        "    def compute_size(self):\n",
        "        # We'll define this later\n",
        "        pass\n",
        "\n",
        "    def time_pipeline(self):\n",
        "        # We'll define this later\n",
        "        pass\n",
        "\n",
        "    def run_benchmark(self):\n",
        "        metrics = {}\n",
        "        metrics[self.optim_type] = self.compute_size()\n",
        "        metrics[self.optim_type].update(self.time_pipeline())\n",
        "        metrics[self.optim_type].update(self.compute_accuracy())\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "83c7ad636fb142a98226c974a6248fe9",
            "0b7121ed5b5640f29a9a8437631068d0",
            "331e4a742c864e5496b10fd4a0588d86",
            "",
            "607e6702bf3043b1bfde3e8eb760021f"
          ]
        },
        "id": "2zRLbOUc-TSu",
        "outputId": "f62ce007-c978-46d3-acbf-b45299b618b8"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from datasets import load_dataset\n",
        "\n",
        "clinc = load_dataset(\"clinc_oos\", \"plus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuajUiIu-TSu",
        "outputId": "04844112-ca3b-43a8-a9a9-e29f9179cf6c"
      },
      "outputs": [],
      "source": [
        "sample = clinc[\"test\"][42]\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUOGWotM-TSu",
        "outputId": "45c700b3-fbda-4dbb-e4be-53871a88e198"
      },
      "outputs": [],
      "source": [
        "intents = clinc[\"test\"].features[\"intent\"]\n",
        "intents.int2str(sample[\"intent\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e24b9b0253964af0b0a891e4ea8859d0"
          ]
        },
        "id": "LybR2NAR-TSv",
        "outputId": "245798f4-d5ab-4b31-a127-7d344fd2aae5"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from datasets import load_metric\n",
        "\n",
        "accuracy_score = load_metric(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcNRfZaW-TSv"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(self):\n",
        "    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
        "    preds, labels = [], []\n",
        "    for example in self.dataset:\n",
        "        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
        "        label = example[\"intent\"]\n",
        "        preds.append(intents.str2int(pred))\n",
        "        labels.append(label)\n",
        "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
        "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
        "    return accuracy\n",
        "\n",
        "PerformanceBenchmark.compute_accuracy = compute_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pztubRh-TSv",
        "outputId": "e14b4835-f49e-42d7-e25c-474d93683386"
      },
      "outputs": [],
      "source": [
        "list(pipe.model.state_dict().items())[42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNgW2c0J-TSv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def compute_size(self):\n",
        "    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n",
        "    state_dict = self.pipeline.model.state_dict()\n",
        "    tmp_path = Path(\"model.pt\")\n",
        "    torch.save(state_dict, tmp_path)\n",
        "    # Calculate size in megabytes\n",
        "    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
        "    # Delete temporary file\n",
        "    tmp_path.unlink()\n",
        "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "    return {\"size_mb\": size_mb}\n",
        "\n",
        "PerformanceBenchmark.compute_size = compute_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrMz807D-TSw",
        "outputId": "1424c21e-ba19-43ee-c6ad-91ce88412545"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "for _ in range(3):\n",
        "    start_time = perf_counter()\n",
        "    _ = pipe(query)\n",
        "    latency = perf_counter() - start_time\n",
        "    print(f\"Latency (ms) - {1000 * latency:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE9xh3dy-TSw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
        "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n",
        "    latencies = []\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = self.pipeline(query)\n",
        "    # Timed run\n",
        "    for _ in range(100):\n",
        "        start_time = perf_counter()\n",
        "        _ = self.pipeline(query)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "    # Compute run statistics\n",
        "    time_avg_ms = 1000 * np.mean(latencies)\n",
        "    time_std_ms = 1000 * np.std(latencies)\n",
        "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
        "\n",
        "PerformanceBenchmark.time_pipeline = time_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqSWF3CE-TSw",
        "outputId": "147c296d-ac91-4316-d727-9e56fa9af437"
      },
      "outputs": [],
      "source": [
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
        "perf_metrics = pb.run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoN1YSyT-TSw"
      },
      "source": [
        "## Making Models Smaller via Knowledge Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGG9MR8m-TSx"
      },
      "source": [
        "### Knowledge Distillation for Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-9vjpNw-TSx"
      },
      "source": [
        "<img alt=\"Soft Probabilities\" caption=\"Comparison of a hard label that is one-hot encoded (left), softmax probabilities (middle), and softened class probabilities (right)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_soft-probs.png?raw=1\" id=\"soft-probs\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7eOYDyi-TSx"
      },
      "source": [
        "<img alt=\"Knowledge distillation\" caption=\"The knowledge distillation process\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_kd.png?raw=1\" id=\"kd\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CP_rX9a-TSx"
      },
      "source": [
        "### Knowledge Distillation for Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kwcFo45-TSx"
      },
      "source": [
        "### Creating a Knowledge Distillation Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTEumOly-TSx"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "class DistillationTrainingArguments(TrainingArguments):\n",
        "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mY5Nr8C-TSx"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer\n",
        "\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inputs = inputs.to(device)\n",
        "        outputs_stu = model(**inputs)\n",
        "        # Extract cross-entropy loss and logits from student\n",
        "        loss_ce = outputs_stu.loss\n",
        "        logits_stu = outputs_stu.logits\n",
        "        # Extract logits from teacher\n",
        "        with torch.no_grad():\n",
        "            outputs_tea = self.teacher_model(**inputs)\n",
        "            logits_tea = outputs_tea.logits\n",
        "        # Soften probabilities and compute distillation loss\n",
        "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
        "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
        "            F.softmax(logits_tea / self.args.temperature, dim=-1))\n",
        "        # Return weighted student loss\n",
        "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
        "        return (loss, outputs_stu) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y4s5p_S-TSx"
      },
      "source": [
        "### Choosing a Good Student Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9cbc667acbd944f593b83c21c8889ce8",
            "c35ba476d0e44ccbb15409660bfe1642",
            "a1a17d9f5cb147c59d1da623957ed4d6",
            "50d7ff524239455292ce226c129b9c28",
            "4b3300485702445aba4a28f61e9678bc",
            "19c19d8e651e4ce1ade55f2fd6170c77",
            "4461fcb4d0e743ed84a0814a642ba73b"
          ]
        },
        "id": "eM0gBrc1-TSy",
        "outputId": "1bd4306d-5b0e-4608-f13c-16ac09c7f79b"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "student_ckpt = \"distilbert-base-uncased\"\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
        "\n",
        "def tokenize_text(batch):\n",
        "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
        "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tFSkVKR-TSy"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43YdzQ99-TSy"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy_score.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cenXYmqV-TSy"
      },
      "outputs": [],
      "source": [
        "batch_size = 48\n",
        "\n",
        "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
        "student_training_args = DistillationTrainingArguments(\n",
        "    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=5, learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\n",
        "    push_to_hub=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npnPW7vO-TSz"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "student_training_args.logging_steps = len(clinc_enc['train']) // batch_size\n",
        "student_training_args.disable_tqdm = False\n",
        "student_training_args.save_steps = 1e9\n",
        "student_training_args.log_level = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZJCG2f9-TSz",
        "outputId": "17ae285c-e7b1-4a75-a481-55c45dd41233"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "%env TOKENIZERS_PARALLELISM=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTUb4uNp-TSz"
      },
      "outputs": [],
      "source": [
        "id2label = pipe.model.config.id2label\n",
        "label2id = pipe.model.config.label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIZyadBR-TSz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "num_labels = intents.num_classes\n",
        "student_config = (AutoConfig\n",
        "                  .from_pretrained(student_ckpt, num_labels=num_labels,\n",
        "                                   id2label=id2label, label2id=label2id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiIK8XjY-TSz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def student_init():\n",
        "    return (AutoModelForSequenceClassification\n",
        "            .from_pretrained(student_ckpt, config=student_config).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0tJ12lf-TS0"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
        "teacher_model = (AutoModelForSequenceClassification\n",
        "                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\n",
        "                 .to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOMKI6Sx-TS4"
      },
      "outputs": [],
      "source": [
        "distilbert_trainer = DistillationTrainer(model_init=student_init,\n",
        "    teacher_model=teacher_model, args=student_training_args,\n",
        "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
        "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
        "\n",
        "distilbert_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZZbDXdr-TS4",
        "outputId": "955aa145-6709-4c2d-a755-fcc321a079bb"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "distilbert_trainer.push_to_hub(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnjGmyRA-TS4"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "finetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=finetuned_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITzB7ixX-TS4",
        "outputId": "0e5c193d-863e-4fce-d95a-307539a40ee7"
      },
      "outputs": [],
      "source": [
        "optim_type = \"DistilBERT\"\n",
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po7QDD_3-TS5",
        "outputId": "a59ffd3d-acf2-4574-8e7b-e24575a0ee00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def plot_metrics(perf_metrics, current_optim_type):\n",
        "    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n",
        "\n",
        "    for idx in df.index:\n",
        "        df_opt = df.loc[idx]\n",
        "        # Add a dashed circle around the current optimization type\n",
        "        if idx == current_optim_type:\n",
        "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
        "                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\n",
        "                        marker='$\\u25CC$')\n",
        "        else:\n",
        "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
        "                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n",
        "\n",
        "    legend = plt.legend(bbox_to_anchor=(1,1))\n",
        "    for handle in legend.legendHandles:\n",
        "        handle.set_sizes([20])\n",
        "\n",
        "    plt.ylim(80,90)\n",
        "    # Use the slowest model to define the x-axis range\n",
        "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
        "    plt.xlim(1, xlim)\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.xlabel(\"Average latency (ms)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(perf_metrics, optim_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDs3euC0-TS5"
      },
      "source": [
        "### Finding Good Hyperparameters with Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYu7KZw3-TS5",
        "outputId": "27f5fe84-0f08-43ea-e014-4d2661b02c93"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "#id banana-function\n",
        "#alt A banana plot\n",
        "#caption Plot of the Rosenbrock function of two variables\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def f(x, y):\n",
        "    return (1-x)**2+100*(y-x**2)**2\n",
        "\n",
        "X, Y = np.meshgrid(np.linspace(-2, 2, 250), np.linspace(-1, 3, 250))\n",
        "Z = f(X,Y)\n",
        "_, ax = plt.subplots()\n",
        "ax.plot([1], [1], 'x', mew=3, markersize=10, color=\"red\")\n",
        "ax.contourf(X, Y, Z, np.logspace(-1, 3, 30), cmap='viridis', extend=\"both\")\n",
        "ax.set_xlim(-1.3, 1.3)\n",
        "ax.set_ylim(-0.9, 1.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMYUQxe_-TS5"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    x = trial.suggest_float(\"x\", -2, 2)\n",
        "    y = trial.suggest_float(\"y\", -2, 2)\n",
        "    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu2nwqKY-TS5"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "import optuna\n",
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFq5Ani--TS6",
        "outputId": "4d7a33c1-10c2-4e63-ec05-34dd57166cb4"
      },
      "outputs": [],
      "source": [
        "study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6ixp1f1-TS6"
      },
      "outputs": [],
      "source": [
        "def hp_space(trial):\n",
        "    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
        "        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP78-oYT-TS6"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "best_run = distilbert_trainer.hyperparameter_search(\n",
        "    n_trials=20, direction=\"maximize\", hp_space=hp_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toJ9WOaw-TS6",
        "outputId": "2c19cbfc-a60a-4e8f-bc8f-9962dfadc78d"
      },
      "outputs": [],
      "source": [
        "print(best_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDMp6quB-TS6",
        "outputId": "7645a8af-c50d-4b9d-f55d-503659216bb4"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "for k,v in best_run.hyperparameters.items():\n",
        "    setattr(student_training_args, k, v)\n",
        "\n",
        "# Define a new repository to store our distilled model\n",
        "distilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\n",
        "student_training_args.output_dir = distilled_ckpt\n",
        "\n",
        "# Create a new Trainer with optimal parameters\n",
        "distil_trainer = DistillationTrainer(model_init=student_init,\n",
        "    teacher_model=teacher_model, args=student_training_args,\n",
        "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
        "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
        "\n",
        "distil_trainer.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTY2u8OM-TS6"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "distil_trainer.push_to_hub(\"Training complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZn7_iF_-TS7"
      },
      "source": [
        "### Benchmarking Our Distilled Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNXTxI4c-TS7",
        "outputId": "0fb931e3-0c65-40d3-8144-2d2b04237f7a"
      },
      "outputs": [],
      "source": [
        "distilled_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=distilled_ckpt)\n",
        "optim_type = \"Distillation\"\n",
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1u3CV2x-TS7",
        "outputId": "a227c610-4c5b-4edf-e091-d54896ad0618"
      },
      "outputs": [],
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpszY-e-TS7"
      },
      "source": [
        "## Making Models Faster with Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kuM9f0O-TS7"
      },
      "source": [
        "### Sidebar: A Primer on Floating-Point and Fixed-Point Numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6_9Oc3-TS7"
      },
      "source": [
        "### End sidebar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct9SLFLa-TS8"
      },
      "source": [
        "<img alt=\"Mapping floating-point numbers to 8-bit integers\" width=\"800\" caption=\"Quantizing floating-point numbers as unsigned 8-bit integers (courtesy of Manas Sahni)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_fp32-to-int8.png?raw=1\" id=\"fp32toint8\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV8y_Xj0-TS8",
        "outputId": "670dec21-d685-4a4e-91f1-f02c7cd9fd3c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "state_dict = pipe.model.state_dict()\n",
        "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
        "plt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXtkRAI2-TS8"
      },
      "outputs": [],
      "source": [
        "zero_point = 0\n",
        "scale = (weights.max() - weights.min()) / (127 - (-128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alFFZWeg-TS8",
        "outputId": "8790ec8b-86c1-476b-d537-37b10c542854"
      },
      "outputs": [],
      "source": [
        "(weights / scale + zero_point).clamp(-128, 127).round().char()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlZE1JO1-TS8",
        "outputId": "7009b433-8e32-4d9e-d911-53a8a5110a66"
      },
      "outputs": [],
      "source": [
        "from torch import quantize_per_tensor\n",
        "\n",
        "dtype = torch.qint8\n",
        "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\n",
        "quantized_weights.int_repr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGHh3uyW-TS8",
        "outputId": "f50af91e-2172-41d3-d4c2-4bb61a1896b1"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "#id weight-quantization\n",
        "#alt Effect of quantization on a transformer's weights\n",
        "#caption Effect of quantization on a transformer's weights\n",
        "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n",
        "\n",
        "# Create histogram\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(quantized_weights.dequantize().flatten().numpy(),\n",
        "         bins=250, range=(-0.3,0.3), edgecolor=\"C0\");\n",
        "# Create zoom inset\n",
        "axins = zoomed_inset_axes(ax, 5, loc='upper right')\n",
        "axins.hist(quantized_weights.dequantize().flatten().numpy(),\n",
        "         bins=250, range=(-0.3,0.3));\n",
        "x1, x2, y1, y2 = 0.05, 0.1, 500, 2500\n",
        "axins.set_xlim(x1, x2)\n",
        "axins.set_ylim(y1, y2)\n",
        "axins.axes.xaxis.set_visible(False)\n",
        "axins.axes.yaxis.set_visible(False)\n",
        "mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8gliHIt-TS9",
        "outputId": "ed40c4c7-3532-419b-c6cc-36c62d4b3787"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "weights @ weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISX3DP3f-TS9"
      },
      "outputs": [],
      "source": [
        "from torch.nn.quantized import QFunctional\n",
        "\n",
        "q_fn = QFunctional()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSWojiyi-TS9",
        "outputId": "4a43fff6-5773-4ff8-a350-e301ef0a6e15"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "q_fn.mul(quantized_weights, quantized_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY-8hSyo-TS9",
        "outputId": "12c1a5b0-0db1-4645-e2bb-d7bca59baeb6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiWYPBYo-TS9"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = (AutoModelForSequenceClassification\n",
        "         .from_pretrained(model_ckpt).to(\"cpu\"))\n",
        "\n",
        "model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWMM8XX0-TS9"
      },
      "source": [
        "### Benchmarking Our Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XiDZkXU-TS-",
        "outputId": "aa902de5-3caf-46f2-a6a7-f10c866a939f"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-classification\", model=model_quantized,\n",
        "                tokenizer=tokenizer)\n",
        "optim_type = \"Distillation + quantization\"\n",
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YGkgcsC-TS-",
        "outputId": "6df94dd5-48bc-42bd-ada4-0d5275a3fcce"
      },
      "outputs": [],
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2MeLszQ-TS-"
      },
      "source": [
        "## Optimizing Inference with ONNX and the ONNX Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3cor75r-TS-"
      },
      "source": [
        "<img alt=\"Example ONNX graph\" width=\"500\" caption=\"A section of the ONNX graph for BERT-base, visualized in Netron\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_bert-onnx.png?raw=1\" id=\"bert-onnx\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvNR7Y2Q-TS-"
      },
      "source": [
        "<img alt=\"Architecture of the ONNX and ONNX Runtime ecosystem\" width=\"500\" caption=\"Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of the ONNX Runtime team)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_onnx-ort.png?raw=1\" id=\"onnx-ort\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ajm6KuTn-TS-",
        "outputId": "5c18179a-cdda-47a8-a647-a8e55b44dd5d"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "import os\n",
        "from psutil import cpu_count\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\n",
        "os.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u4sg4m0-TS-",
        "outputId": "c3ad36e1-7439-40e8-9c0d-fdd1b1a35fed"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from transformers.convert_graph_to_onnx import convert\n",
        "\n",
        "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
        "onnx_model_path = Path(\"onnx/model.onnx\")\n",
        "convert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\n",
        "        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMd3SZjg-TS-"
      },
      "outputs": [],
      "source": [
        "from onnxruntime import (GraphOptimizationLevel, InferenceSession,\n",
        "                         SessionOptions)\n",
        "\n",
        "def create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\n",
        "    options = SessionOptions()\n",
        "    options.intra_op_num_threads = 1\n",
        "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
        "    session.disable_fallback()\n",
        "    return session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BLO-8wE-TS_"
      },
      "outputs": [],
      "source": [
        "onnx_model = create_model_for_provider(onnx_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3KxFk5U-TS_",
        "outputId": "d0ce6496-bd85-4663-cee6-2894af4ed18c"
      },
      "outputs": [],
      "source": [
        "inputs = clinc_enc[\"test\"][:1]\n",
        "del inputs[\"labels\"]\n",
        "logits_onnx = onnx_model.run(None, inputs)[0]\n",
        "logits_onnx.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KWVSQUT-TS_",
        "outputId": "4a4ed151-5d79-46db-bd16-9e0f819cd1fd"
      },
      "outputs": [],
      "source": [
        "np.argmax(logits_onnx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiZV8Jxd-TS_",
        "outputId": "be41a886-b434-4ed5-ca22-c98e2c0ff87b"
      },
      "outputs": [],
      "source": [
        "clinc_enc[\"test\"][0][\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNjUs99-TS_"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class OnnxPipeline:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, query):\n",
        "        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n",
        "        inputs_onnx = {k: v.cpu().detach().numpy()\n",
        "                       for k, v in model_inputs.items()}\n",
        "        logits = self.model.run(None, inputs_onnx)[0][0, :]\n",
        "        probs = softmax(logits)\n",
        "        pred_idx = np.argmax(probs).item()\n",
        "        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoCUZy0U-TS_",
        "outputId": "5e281155-7a3c-431b-843c-4a930ab8938d"
      },
      "outputs": [],
      "source": [
        "pipe = OnnxPipeline(onnx_model, tokenizer)\n",
        "pipe(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLOdkyKo-TTA"
      },
      "outputs": [],
      "source": [
        "class OnnxPerformanceBenchmark(PerformanceBenchmark):\n",
        "    def __init__(self, *args, model_path, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def compute_size(self):\n",
        "        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n",
        "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "        return {\"size_mb\": size_mb}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jyzfG-a-TTA",
        "outputId": "c7992d5e-e34c-440b-d642-04dcf3d82f4f"
      },
      "outputs": [],
      "source": [
        "optim_type = \"Distillation + ORT\"\n",
        "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n",
        "                              model_path=\"onnx/model.onnx\")\n",
        "perf_metrics.update(pb.run_benchmark())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLxgbaP8-TTA",
        "outputId": "97ae96a9-b4cf-4c54-df17-f68b112c375d"
      },
      "outputs": [],
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7qE7dxS-TTB"
      },
      "outputs": [],
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_input = \"onnx/model.onnx\"\n",
        "model_output = \"onnx/model.quant.onnx\"\n",
        "quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcYpEnYH-TTB",
        "outputId": "efcf0be0-ee6d-44b4-ced7-60849c515f6b"
      },
      "outputs": [],
      "source": [
        "onnx_quantized_model = create_model_for_provider(model_output)\n",
        "pipe = OnnxPipeline(onnx_quantized_model, tokenizer)\n",
        "optim_type = \"Distillation + ORT (quantized)\"\n",
        "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n",
        "                              model_path=model_output)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59zVHWNe-TTB",
        "outputId": "d64a6fc9-4113-47f4-944e-6637793e695a"
      },
      "outputs": [],
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WeejVJ-TTB"
      },
      "source": [
        "## Making Models Sparser with Weight Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjACbL0O-TTB"
      },
      "source": [
        "### Sparsity in Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvQn5oWS-TTC"
      },
      "source": [
        "<img alt=\"Network Pruning\" width=\"500\" caption=\"Weights and neurons before and after pruning (courtesy of Song Han)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_network-pruning.png?raw=1\" id=\"network-pruning\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bRa1-nQ-TTC"
      },
      "source": [
        "### Weight Pruning Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjEsPvk2-TTC"
      },
      "source": [
        "#### Magnitude pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucf3VydT-TTC",
        "outputId": "f72af85c-b0c5-46f1-c7a6-05a7ac5f667c"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "#id sparsity-scheduler\n",
        "#alt Sparsity scheduler\n",
        "#caption The cubic sparsity scheduler used for pruning\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _sparsity(t, t_0=0, dt=1, s_i=0, s_f=0.9, N=100):\n",
        "    return s_f + (s_i - s_f) * (1 - (t - t_0) / (N * dt))**3\n",
        "\n",
        "steps = np.linspace(0,100,100)\n",
        "values = [_sparsity(t) for t in steps]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps, values)\n",
        "ax.set_ylim(0,1)\n",
        "ax.set_xlim(0,100)\n",
        "ax.set_xlabel(\"Pruning step\")\n",
        "ax.set_ylabel(\"Sparsity\")\n",
        "plt.grid(linestyle=\"dashed\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To1IG_Ua-TTC"
      },
      "source": [
        "#### Movement pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46DTx2XO-TTD"
      },
      "source": [
        "<img alt=\"Magnitude vs Movement Pruning\" width=\"700\" caption=\"Comparison of weights removed (in gray) during magnitude pruning (left) and movement pruning (right)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_magnitude-vs-movement.png?raw=1\" id=\"magnitude-vs-movement\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4aa7_wI-TTD"
      },
      "source": [
        "<img alt=\"Pruning Distributions\" width=\"500\" caption=\"Distribution of remaining weights for magnitude pruning (MaP) and movement pruning (MvP)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_pruning-dists.png?raw=1\" id=\"pruning-dists\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYdFLS0H-TTD"
      },
      "source": [
        "## Conclusion"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
