{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8d963baedca40aaa3c6c3e4e896b9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f59dbfdf23b4442b75e146026ea0a6f",
              "IPY_MODEL_10fda72bbfd84bc78832d01277517294",
              "IPY_MODEL_cd64a466b2fa40969ae2409faef6dd39"
            ],
            "layout": "IPY_MODEL_efaa0effd0ae4835a8d1629efba2540b"
          }
        },
        "5f59dbfdf23b4442b75e146026ea0a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cc9be08f207437ebb26f45e4862c521",
            "placeholder": "​",
            "style": "IPY_MODEL_61a800d78a2d44b6ad7109670d329d3d",
            "value": "Map: 100%"
          }
        },
        "10fda72bbfd84bc78832d01277517294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecce476965fb41abbab80e55782a0465",
            "max": 5500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33bc5fe30c014a3faede3cfb9353d51e",
            "value": 5500
          }
        },
        "cd64a466b2fa40969ae2409faef6dd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a69620c126548fb9906ea1ebac548d7",
            "placeholder": "​",
            "style": "IPY_MODEL_1bb4c5e9c9f94e409485dc81cc639494",
            "value": " 5500/5500 [00:00&lt;00:00, 16263.23 examples/s]"
          }
        },
        "efaa0effd0ae4835a8d1629efba2540b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc9be08f207437ebb26f45e4862c521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a800d78a2d44b6ad7109670d329d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecce476965fb41abbab80e55782a0465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33bc5fe30c014a3faede3cfb9353d51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a69620c126548fb9906ea1ebac548d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb4c5e9c9f94e409485dc81cc639494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/NLP_08_model_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/simonmellergaard/nlp-08-model-compression/edit)"
      ],
      "metadata": {
        "id": "nZKo9tP3A0W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run this cell if you're on Colab or Kaggle\n",
        "# !git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "# %cd notebooks\n",
        "# from install import *\n",
        "# install_requirements()"
      ],
      "metadata": {
        "id": "gmrSz3Cs-TSn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# hide\n",
        "# from utils import *\n",
        "# setup_chapter()"
      ],
      "metadata": {
        "id": "ihNxoKmy-TSo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbPuT0fiH9gx",
        "outputId": "1038fd94-3dfb-4f4f-c38b-dfacaa69274e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Transformers Efficient in Production"
      ],
      "metadata": {
        "id": "fee5oYPb-TSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Scaling BERT at Roblox\" caption=\"How Roblox scaled BERT with knowledge distillation, dynamic padding, and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip Kaehler)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_roblox.png?raw=1\" id=\"roblox\"/>"
      ],
      "metadata": {
        "id": "P9tXBzWz-TSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intent Detection as a Case Study"
      ],
      "metadata": {
        "id": "FbpvJ_dg-TSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Out of Scope Query\" width=\"400\" caption=\"Three exchanges between a human (right) and a text-based assistant (left) for personal finance (courtesy of Stefan Larson et al.)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_oos.png?raw=1\" id=\"oos\"/>"
      ],
      "metadata": {
        "id": "2Nr7YJz0-TSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "from transformers import pipeline\n",
        "\n",
        "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=bert_ckpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENOzILTA-TSs",
        "outputId": "997bf572-2cd3-4aaa-de9e-ccc0dc6a6ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in\n",
        "Paris and I need a 15 passenger van\"\"\"\n",
        "pipe(query)"
      ],
      "metadata": {
        "id": "uuO7aVHv-TSt",
        "outputId": "733457c8-4488-47d0-cd48-6ee857b048b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'car_rental', 'score': 0.5490034222602844}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Performance Benchmark"
      ],
      "metadata": {
        "id": "iYFP4Eeh-TSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceBenchmark:\n",
        "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
        "        self.pipeline = pipeline\n",
        "        self.dataset = dataset\n",
        "        self.optim_type = optim_type\n",
        "\n",
        "    def compute_accuracy(self):\n",
        "        # We'll define this later\n",
        "        pass\n",
        "\n",
        "    def compute_size(self):\n",
        "        # We'll define this later\n",
        "        pass\n",
        "\n",
        "    def time_pipeline(self):\n",
        "        # We'll define this later\n",
        "        pass\n",
        "\n",
        "    def run_benchmark(self):\n",
        "        metrics = {}\n",
        "        metrics[self.optim_type] = self.compute_size()\n",
        "        metrics[self.optim_type].update(self.time_pipeline())\n",
        "        metrics[self.optim_type].update(self.compute_accuracy())\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "QMb_lP6m-TSt"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "clinc = load_dataset(\"clinc_oos\", \"plus\")"
      ],
      "metadata": {
        "id": "2zRLbOUc-TSu"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "sample = clinc[\"test\"][42]\n",
        "sample"
      ],
      "metadata": {
        "id": "LuajUiIu-TSu",
        "outputId": "fd69c5d4-e9cd-40fd-9502-195d4dbc60d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "intents = clinc[\"test\"].features[\"intent\"]\n",
        "intents.int2str(sample[\"intent\"])"
      ],
      "metadata": {
        "id": "wUOGWotM-TSu",
        "outputId": "8318c347-6e22-42b5-c5d5-b2606b4577d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'transfer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_score = evaluate.load('accuracy')"
      ],
      "metadata": {
        "id": "LybR2NAR-TSv"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(self):\n",
        "    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
        "    preds, labels = [], []\n",
        "    for example in self.dataset:\n",
        "        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
        "        label = example[\"intent\"]\n",
        "        preds.append(intents.str2int(pred))\n",
        "        labels.append(label)\n",
        "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
        "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
        "    return accuracy\n",
        "\n",
        "PerformanceBenchmark.compute_accuracy = compute_accuracy"
      ],
      "metadata": {
        "id": "lcNRfZaW-TSv"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "list(pipe.model.state_dict().items())[42]"
      ],
      "metadata": {
        "id": "_pztubRh-TSv",
        "outputId": "baf57e38-fb63-4fb4-9990-b15a874e8265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bert.encoder.layer.2.attention.self.value.bias',\n",
              " tensor([-2.7834e-02,  4.9434e-02,  8.3551e-02,  4.1092e-02,  6.0157e-01,\n",
              "          1.1774e-01, -5.2112e-02, -6.5143e-02, -2.9358e-02, -4.2250e-02,\n",
              "          7.9177e-02,  8.0409e-02,  2.9921e-03,  1.7816e-01, -5.0480e-02,\n",
              "         -1.5634e-01, -2.1707e-02,  1.4381e-02,  2.5132e-02, -2.4110e-02,\n",
              "         -1.9183e-01, -7.8657e-02,  5.0709e-02,  3.3632e-02, -3.1946e-02,\n",
              "          1.1616e-01,  9.2720e-02, -1.1787e-01,  2.3233e-01, -1.2678e-02,\n",
              "         -1.3138e-01, -4.0024e-02,  7.4823e-02, -5.4148e-02, -1.5184e-01,\n",
              "         -7.4407e-02,  1.1559e-01,  8.2729e-02, -1.3787e-01,  8.3528e-02,\n",
              "          1.2154e-01,  1.6880e-02, -5.6629e-02, -3.9295e-02,  5.3725e-02,\n",
              "          6.8602e-02, -1.1294e-01,  4.4001e-02, -2.5884e-01,  1.6767e-01,\n",
              "          1.8316e-01,  5.6272e-02, -3.6874e-02, -2.7938e-02, -9.3204e-02,\n",
              "         -7.5239e-03,  4.1141e-02, -1.1542e-02, -9.9749e-02, -3.0910e-02,\n",
              "          4.1398e-02, -4.4389e-02, -2.6279e-02,  7.2100e-02,  7.5179e-03,\n",
              "         -7.4382e-03,  2.9311e-02, -1.3391e-02,  6.9966e-03, -9.3249e-03,\n",
              "          9.4272e-03, -1.1783e-02,  1.3849e-02,  1.8157e-03, -1.1522e-02,\n",
              "          1.3364e-02, -2.6307e-02,  2.3725e-03, -4.8451e-03,  6.2261e-03,\n",
              "          1.2653e-02,  1.7601e-02, -1.7971e-02, -2.9247e-03, -3.3447e-03,\n",
              "          1.4263e-02, -3.5629e-03, -9.2794e-03, -2.1326e-02,  1.9390e-02,\n",
              "          1.3287e-02, -8.7034e-03,  1.2936e-02, -2.0574e-02,  3.2204e-03,\n",
              "          5.9970e-03, -5.6524e-02,  3.0851e-02, -2.3233e-02,  4.6271e-02,\n",
              "         -1.4485e-03,  4.4248e-04, -3.1102e-02,  1.9762e-02,  2.0866e-02,\n",
              "          1.7914e-02, -2.0622e-02, -1.6030e-02,  6.2167e-03,  1.6809e-02,\n",
              "          4.6357e-03,  4.7169e-02, -2.1151e-02, -1.8898e-02,  3.5921e-02,\n",
              "          4.8621e-03, -5.9841e-02, -1.3029e-02,  5.6702e-03,  1.6820e-02,\n",
              "          2.1735e-02, -2.8285e-03, -1.5519e-02,  7.1974e-03,  3.5492e-02,\n",
              "         -2.3190e-02, -9.7930e-03,  2.3223e-02,  2.0529e-02,  2.7412e-02,\n",
              "          1.3887e-02,  1.7976e-03, -3.8905e-02,  1.8540e-02,  8.0013e-03,\n",
              "         -8.3682e-03, -8.6805e-03,  2.6464e-02,  1.8968e-03, -1.8028e-03,\n",
              "         -2.4427e-02, -3.5514e-02, -1.8748e-02, -3.0541e-02, -7.9520e-03,\n",
              "         -1.5989e-02,  9.3060e-03,  2.9318e-02,  1.6679e-02, -2.4060e-03,\n",
              "         -7.2656e-03, -2.7699e-03,  1.4280e-02, -6.7352e-02, -1.2329e-02,\n",
              "         -1.1516e-02, -8.0379e-03,  2.5604e-02, -2.9178e-02,  2.6076e-02,\n",
              "         -1.1561e-02, -4.0904e-03, -1.7735e-02, -2.8245e-03,  2.3919e-02,\n",
              "         -3.6326e-03, -1.6407e-03,  9.5295e-03,  2.4161e-02, -5.8965e-02,\n",
              "          4.7633e-02,  1.4403e-03, -4.4763e-02, -3.5462e-03,  3.0613e-02,\n",
              "          2.5047e-02,  1.3757e-02, -2.2238e-02,  8.0453e-03, -2.6188e-03,\n",
              "         -2.2013e-03, -6.0179e-03, -8.5149e-03, -2.8150e-02,  1.9282e-02,\n",
              "         -7.5657e-02,  6.1354e-03,  5.2562e-03, -2.2223e-03,  1.0437e-02,\n",
              "         -1.8630e-02, -7.7628e-02,  1.4377e-02,  1.4977e-02,  1.6090e-02,\n",
              "          2.9399e-02, -2.8604e-02, -3.2916e-02,  2.7902e-03,  1.4113e-02,\n",
              "          8.3836e-03, -6.5914e-03, -4.9576e-03, -1.5955e-02,  4.2381e-03,\n",
              "          3.0032e-02,  8.8986e-03, -1.5336e-02,  4.6771e-03, -1.2364e-02,\n",
              "         -3.7724e-02, -3.9060e-03,  1.4607e-02, -2.1286e-02,  9.7086e-03,\n",
              "         -1.5826e-02, -1.4847e-02,  1.0796e-03, -9.5035e-03, -1.8624e-02,\n",
              "         -3.1673e-02,  8.8388e-03, -1.4921e-02, -1.7855e-03,  3.8781e-02,\n",
              "          2.4269e-02, -1.6465e-03, -7.9774e-03, -1.3907e-02,  1.9653e-03,\n",
              "         -2.6717e-03,  3.0146e-02, -4.3986e-03,  6.0588e-03,  2.1959e-02,\n",
              "          2.0354e-02,  9.3631e-03, -8.4828e-03, -2.3994e-02,  1.8316e-02,\n",
              "         -2.2401e-02, -2.6273e-03,  3.2069e-03, -3.2386e-03,  1.1388e-02,\n",
              "          3.0835e-02,  2.6043e-02, -7.2103e-03, -5.0038e-04, -1.4860e-02,\n",
              "          2.3886e-02,  1.1530e-02, -1.0549e-03,  3.5677e-02,  6.0299e-03,\n",
              "          1.5875e-02,  5.3877e-02,  6.6331e-02,  1.5165e-02, -6.0870e-03,\n",
              "          1.6159e-03, -1.1137e-02,  1.3665e-02,  1.7698e-02,  4.5647e-03,\n",
              "         -4.5349e-02,  2.6634e-02,  4.7289e-03, -1.1664e-02,  8.7302e-03,\n",
              "          1.2051e-02,  2.3455e-02, -2.7151e-03, -1.3474e-02, -4.5949e-02,\n",
              "         -1.3496e-02,  8.1680e-02, -2.9737e-02, -2.8525e-02, -1.4888e-02,\n",
              "          5.4320e-03,  3.9142e-02,  1.4227e-02,  1.1140e-02, -1.0980e-02,\n",
              "          2.1982e-02, -1.1148e-02, -1.0226e-02, -5.8499e-03, -1.9739e-02,\n",
              "         -3.4634e-03, -1.0565e-02,  1.0449e-02,  8.2791e-03,  2.3053e-03,\n",
              "          9.7032e-03, -9.0916e-03,  5.4747e-04, -4.1901e-02,  9.6154e-03,\n",
              "          2.9424e-02,  1.1897e-03,  1.1825e-02,  2.7524e-02,  1.0225e-02,\n",
              "          2.1439e-02,  2.3448e-03, -3.7239e-02,  5.8888e-02, -3.1999e-02,\n",
              "         -7.9128e-03,  3.5337e-02,  2.7251e-02,  1.3948e-02,  1.0896e-02,\n",
              "          1.4326e-05, -2.6835e-03,  4.0720e-03,  1.6469e-02,  3.5876e-02,\n",
              "         -1.6803e-02,  2.8083e-02,  2.2789e-03,  3.1635e-02,  5.7104e-03,\n",
              "          3.0284e-03,  1.2528e-02, -1.7956e-02, -1.4686e-02,  4.4091e-03,\n",
              "         -1.4846e-02, -4.1044e-02, -3.4005e-02, -6.5876e-02,  3.0926e-02,\n",
              "          5.9820e-02, -1.0069e-04, -6.6089e-03,  2.1271e-03,  3.6137e-03,\n",
              "          9.0073e-03,  9.6992e-03,  2.1992e-02,  6.2104e-02,  1.4786e-02,\n",
              "         -2.6120e-02, -1.4990e-02,  1.5148e-02,  3.0313e-02,  2.0834e-02,\n",
              "          1.7836e-02, -5.4321e-03, -5.9164e-03,  1.7840e-02, -4.3020e-03,\n",
              "          8.7374e-03, -2.4993e-02,  3.4310e-02,  2.2652e-02, -5.3760e-03,\n",
              "          1.7668e-02, -8.9518e-04,  1.3691e-03, -2.1373e-02, -6.1878e-03,\n",
              "         -1.2396e-02, -1.7816e-02, -1.8014e-02,  9.5274e-03,  1.1643e-02,\n",
              "         -2.0683e-02, -2.8707e-03,  1.1669e-02,  1.5618e-02,  3.5348e-02,\n",
              "         -1.1234e-02, -4.5453e-03, -3.4890e-02, -3.0010e-02,  1.6433e-02,\n",
              "         -1.2068e-02,  8.2583e-03,  1.4090e-02,  1.8771e-02, -4.9337e-02,\n",
              "         -1.7775e-03,  5.6333e-02,  6.8979e-02,  2.3123e-02, -1.0754e-02,\n",
              "         -4.9530e-02,  4.2980e-02,  2.8846e-02, -3.9176e-02,  8.1903e-02,\n",
              "         -2.9344e-02,  3.0343e-02,  8.4269e-02, -2.4376e-02, -8.4680e-02,\n",
              "          6.7452e-03,  1.2407e-01,  1.5680e-02, -1.6643e-02,  9.0987e-03,\n",
              "         -2.7984e-03,  7.5874e-02, -1.4630e-02,  2.9823e-02, -1.0423e-02,\n",
              "         -6.0341e-02, -3.6580e-02, -5.4615e-02, -1.3628e-01, -5.8373e-02,\n",
              "          4.3453e-02,  3.0439e-02,  1.7298e-02, -1.1724e-01,  1.0805e-01,\n",
              "         -4.6345e-02, -1.1533e-01,  7.4684e-02, -5.9752e-03,  4.3189e-02,\n",
              "          4.4795e-02, -3.7566e-03, -7.5097e-02,  3.5959e-02,  1.3906e-01,\n",
              "          4.6041e-02,  1.1786e-02,  1.1243e-01,  8.4072e-02, -7.7309e-02,\n",
              "          1.3586e-02,  9.5244e-02, -1.2002e-01,  7.0882e-02,  1.0043e-01,\n",
              "          2.5874e-02,  2.3354e-02, -3.5469e-02,  3.0738e-02,  1.0479e-01,\n",
              "         -9.2843e-02,  5.9718e-02, -1.9409e-02, -3.4414e-02, -8.4060e-03,\n",
              "         -9.3654e-03, -2.8030e-02,  7.2026e-03, -5.9461e-03, -1.2284e-02,\n",
              "         -1.7473e-02, -4.9678e-02,  1.0224e-02, -1.4688e-02, -1.7345e-02,\n",
              "          2.6771e-02, -2.6582e-02, -2.9768e-02,  6.2005e-03, -3.3405e-04,\n",
              "          9.1245e-03, -3.7149e-02, -6.7714e-03, -1.8193e-02,  3.4191e-02,\n",
              "         -5.5732e-03, -2.6161e-02, -1.4078e-02,  1.0288e-02, -2.2850e-02,\n",
              "         -2.2642e-02,  1.0270e-02,  6.9930e-03,  1.3614e-03, -2.9319e-04,\n",
              "          9.9593e-03, -6.0532e-03,  1.8669e-03, -5.3395e-03, -2.2013e-02,\n",
              "          8.4485e-03, -1.9142e-02,  1.0449e-02, -1.6700e-02,  1.3038e-02,\n",
              "          7.7479e-03, -1.3329e-02,  1.3052e-02, -2.5305e-03,  1.6191e-02,\n",
              "          1.3152e-02,  2.9897e-02,  8.5553e-03, -6.4898e-04,  1.2359e-02,\n",
              "         -4.2144e-03,  2.1785e-02,  2.1401e-02, -1.3295e-02,  7.4764e-03,\n",
              "          6.3580e-03, -6.2467e-03,  1.8235e-02,  3.0643e-02, -4.9091e-03,\n",
              "         -1.3889e-02,  2.6181e-03,  4.9702e-03, -6.1664e-03, -2.8528e-03,\n",
              "         -4.0556e-04,  5.4844e-03,  9.7783e-03,  1.2326e-02, -1.6844e-02,\n",
              "         -1.6294e-02,  4.9879e-03,  3.7789e-03,  8.0305e-04,  2.0482e-02,\n",
              "         -1.6369e-02,  1.6436e-03, -9.0401e-03, -3.2635e-02, -2.3990e-03,\n",
              "          5.7569e-03, -2.0392e-02,  6.6946e-03, -1.8541e-02,  9.3559e-03,\n",
              "          2.7165e-02,  1.2492e-02,  1.2748e-03, -1.3846e-03,  4.0312e-03,\n",
              "         -8.5552e-03,  3.1621e-03, -3.9514e-02, -3.7938e-03, -1.3551e-02,\n",
              "          2.2692e-02, -1.0217e-02, -1.2839e-02, -4.8771e-03, -4.2620e-03,\n",
              "         -3.0481e-02,  4.6085e-02, -2.8627e-03,  6.6756e-03, -2.6765e-03,\n",
              "         -4.1443e-02,  1.3336e-02, -1.0526e-02, -3.8494e-02,  1.2780e-02,\n",
              "         -3.5214e-02, -2.4758e-02,  7.5220e-03,  1.7225e-02, -1.6440e-02,\n",
              "         -3.6639e-03, -4.7462e-02,  5.3796e-04, -2.8419e-03,  4.5923e-03,\n",
              "         -3.0517e-02, -3.6832e-03,  1.8710e-02, -3.0486e-02, -2.4076e-02,\n",
              "         -2.7601e-02,  1.1921e-01,  1.2020e-01,  5.9805e-02, -6.9238e-03,\n",
              "         -1.3529e-01,  1.1234e-01,  8.3534e-02,  1.9974e-01,  5.3834e-02,\n",
              "         -6.6691e-02,  1.2676e-01,  2.4947e-01,  4.9879e-01, -1.6342e-01,\n",
              "          6.1663e-02, -4.8022e-02, -5.4069e-02,  4.7277e-02, -1.4724e-01,\n",
              "          8.4666e-02,  9.8618e-02,  7.9988e-02, -5.7652e-02,  6.3551e-03,\n",
              "          2.6916e-03,  9.2017e-02,  1.2511e-01,  2.5168e-01, -8.6152e-02,\n",
              "         -2.7570e-02, -6.1548e-02,  9.1371e-02,  8.0129e-02, -1.1662e-01,\n",
              "          1.5206e-01, -1.0687e-01, -1.0758e-01,  9.4138e-02,  1.0322e-01,\n",
              "         -1.5152e-01,  2.5472e-01,  7.0699e-02, -4.1571e-02,  2.7933e-02,\n",
              "         -1.9523e-01, -2.8319e-01,  5.9689e-02, -1.4707e-01, -3.5683e-02,\n",
              "         -5.4465e-02,  1.2570e-01, -1.2280e-01,  2.0263e-01, -1.0653e-02,\n",
              "          1.1058e-01, -9.1731e-04,  8.6605e-02, -1.7707e-02,  1.2616e-02,\n",
              "          1.8871e-01, -1.0594e-01, -3.7989e-03, -6.6359e-02, -5.9799e-02,\n",
              "          4.3201e-03, -2.3140e-02,  4.8008e-03,  1.4294e-02, -9.5453e-03,\n",
              "          5.8454e-03,  8.6512e-03,  2.1151e-02,  1.7098e-02, -1.4637e-03,\n",
              "          1.3778e-02, -1.2843e-02,  2.3742e-02, -1.3895e-02,  1.1327e-03,\n",
              "         -7.0846e-03, -7.6255e-03, -8.2264e-03, -1.5513e-02,  4.2724e-03,\n",
              "         -1.7624e-02, -3.0984e-03, -1.4400e-02,  2.3883e-02,  1.1556e-01,\n",
              "          4.4036e-02,  3.2821e-02,  8.6612e-04,  1.2383e-02, -6.6263e-03,\n",
              "         -2.7201e-03,  4.8125e-03, -2.8362e-03,  3.9830e-02,  5.2414e-02,\n",
              "         -4.6269e-03, -1.5858e-02,  2.1845e-02, -1.3767e-02, -1.0629e-02,\n",
              "          1.0347e-02, -3.4209e-02,  1.6939e-02,  1.8120e-02,  3.0650e-03,\n",
              "         -2.5109e-03,  1.3500e-02, -3.6440e-02, -9.2135e-03,  9.2194e-03,\n",
              "         -3.6276e-02, -1.1555e-02, -1.9569e-02, -1.5417e-02, -2.0485e-02,\n",
              "          2.9770e-03, -6.8246e-03, -6.5039e-03, -1.2597e-02, -1.1409e-02,\n",
              "          1.7139e-02,  4.6424e-03, -5.1774e-02, -1.1679e-02,  5.2761e-03,\n",
              "         -1.3664e-02, -1.4001e-02,  2.0379e-03,  1.2753e-02, -8.1607e-03,\n",
              "         -1.0849e-02, -1.0683e-03, -2.4489e-03,  2.0192e-02,  1.5756e-02,\n",
              "         -6.4964e-03, -4.5603e-02, -2.5999e-02, -1.4307e-02, -7.4639e-03,\n",
              "          9.9480e-03,  1.6763e-02,  2.1737e-02,  4.8181e-02,  1.7739e-02,\n",
              "          1.1469e-02, -4.2359e-03, -1.6419e-02, -1.7318e-02,  1.1498e-03,\n",
              "          2.4485e-02, -3.0614e-03,  1.9017e-02, -1.6453e-02, -1.3444e-02,\n",
              "          2.6748e-02, -6.4994e-03,  5.1485e-03,  1.4132e-02, -1.1573e-02,\n",
              "          1.2583e-02, -1.1401e-02, -2.0003e-02, -3.8115e-03,  7.3728e-03,\n",
              "          1.5500e-02,  2.0623e-02,  3.1582e-03, -1.2794e-02,  4.5499e-03,\n",
              "         -3.3994e-04, -2.3003e-02,  1.2225e-02, -2.7949e-02, -9.6121e-03,\n",
              "         -3.5844e-02, -4.2373e-03,  5.3473e-03,  1.4432e-02, -1.2745e-02,\n",
              "          7.1483e-03,  2.9170e-03,  6.2454e-03,  6.8991e-03,  1.5852e-02,\n",
              "          2.1877e-02, -1.7211e-02, -2.6093e-02]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def compute_size(self):\n",
        "    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n",
        "    state_dict = self.pipeline.model.state_dict()\n",
        "    tmp_path = Path(\"model.pt\")\n",
        "    torch.save(state_dict, tmp_path)\n",
        "    # Calculate size in megabytes\n",
        "    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
        "    # Delete temporary file\n",
        "    tmp_path.unlink()\n",
        "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "    return {\"size_mb\": size_mb}\n",
        "\n",
        "PerformanceBenchmark.compute_size = compute_size"
      ],
      "metadata": {
        "id": "vNgW2c0J-TSv"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "for _ in range(3):\n",
        "    start_time = perf_counter()\n",
        "    _ = pipe(query)\n",
        "    latency = perf_counter() - start_time\n",
        "    print(f\"Latency (ms) - {1000 * latency:.3f}\")"
      ],
      "metadata": {
        "id": "RrMz807D-TSw",
        "outputId": "efbb4d46-4d4e-487a-f042-842a5436057d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency (ms) - 265.347\n",
            "Latency (ms) - 138.449\n",
            "Latency (ms) - 143.919\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
        "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n",
        "    latencies = []\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = self.pipeline(query)\n",
        "    # Timed run\n",
        "    for _ in range(100):\n",
        "        start_time = perf_counter()\n",
        "        _ = self.pipeline(query)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "    # Compute run statistics\n",
        "    time_avg_ms = 1000 * np.mean(latencies)\n",
        "    time_std_ms = 1000 * np.std(latencies)\n",
        "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
        "\n",
        "PerformanceBenchmark.time_pipeline = time_pipeline"
      ],
      "metadata": {
        "id": "wE9xh3dy-TSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd4fda7-926c-4e0a-afb1-5576c1dda5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:18: SyntaxWarning: invalid escape sequence '\\-'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\-'\n",
            "/tmp/ipython-input-720322860.py:18: SyntaxWarning: invalid escape sequence '\\-'\n",
            "  print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
        "perf_metrics = pb.run_benchmark()"
      ],
      "metadata": {
        "id": "mqSWF3CE-TSw",
        "outputId": "97dba8a5-c769-47f2-80f2-357e680c5d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size (MB) - 418.15\n",
            "Average latency (ms) - 94.41 +\\- 16.83\n",
            "Accuracy on test set - 0.867\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Models Smaller via Knowledge Distillation"
      ],
      "metadata": {
        "id": "XoN1YSyT-TSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge Distillation for Fine-Tuning"
      ],
      "metadata": {
        "id": "ZGG9MR8m-TSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Soft Probabilities\" caption=\"Comparison of a hard label that is one-hot encoded (left), softmax probabilities (middle), and softened class probabilities (right)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_soft-probs.png?raw=1\" id=\"soft-probs\"/>"
      ],
      "metadata": {
        "id": "J-9vjpNw-TSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Knowledge distillation\" caption=\"The knowledge distillation process\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_kd.png?raw=1\" id=\"kd\"/>"
      ],
      "metadata": {
        "id": "M7eOYDyi-TSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge Distillation for Pretraining"
      ],
      "metadata": {
        "id": "2CP_rX9a-TSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Knowledge Distillation Trainer"
      ],
      "metadata": {
        "id": "2kwcFo45-TSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "class DistillationTrainingArguments(TrainingArguments):\n",
        "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature"
      ],
      "metadata": {
        "id": "wTEumOly-TSx"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer\n",
        "\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inputs = inputs.to(device)\n",
        "        outputs_stu = model(**inputs)\n",
        "        # Extract cross-entropy loss and logits from student\n",
        "        loss_ce = outputs_stu.loss\n",
        "        logits_stu = outputs_stu.logits\n",
        "        # Extract logits from teacher\n",
        "        with torch.no_grad():\n",
        "            outputs_tea = self.teacher_model(**inputs)\n",
        "            logits_tea = outputs_tea.logits\n",
        "        # Soften probabilities and compute distillation loss\n",
        "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
        "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
        "            F.softmax(logits_tea / self.args.temperature, dim=-1))\n",
        "        # Return weighted student loss\n",
        "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
        "        return (loss, outputs_stu) if return_outputs else loss"
      ],
      "metadata": {
        "id": "8mY5Nr8C-TSx"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing a Good Student Initialization"
      ],
      "metadata": {
        "id": "4y4s5p_S-TSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "student_ckpt = \"distilbert-base-uncased\"\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
        "\n",
        "def tokenize_text(batch):\n",
        "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
        "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
      ],
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f8d963baedca40aaa3c6c3e4e896b9bc",
            "5f59dbfdf23b4442b75e146026ea0a6f",
            "10fda72bbfd84bc78832d01277517294",
            "cd64a466b2fa40969ae2409faef6dd39",
            "efaa0effd0ae4835a8d1629efba2540b",
            "1cc9be08f207437ebb26f45e4862c521",
            "61a800d78a2d44b6ad7109670d329d3d",
            "ecce476965fb41abbab80e55782a0465",
            "33bc5fe30c014a3faede3cfb9353d51e",
            "5a69620c126548fb9906ea1ebac548d7",
            "1bb4c5e9c9f94e409485dc81cc639494"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "eM0gBrc1-TSy",
        "outputId": "bb19c0e4-fe23-47cf-d2f5-bff710ef5dc6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8d963baedca40aaa3c6c3e4e896b9bc"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging in to Huggingface and wandb\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login as login_hf\n",
        "from wandb import login as login_wandb\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF')\n",
        "os.environ['WANDB_TOKEN'] = userdata.get('wandb')\n",
        "login_hf(os.environ['HF_TOKEN'])\n",
        "login_wandb(key = os.environ['WANDB_TOKEN'])"
      ],
      "metadata": {
        "id": "4tFSkVKR-TSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2849c5-9843-418e-f924-29508b022c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimonmellergaard\u001b[0m (\u001b[33msimonmellergaard-aarhus-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy_score.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "43YdzQ99-TSy"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 48\n",
        "\n",
        "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
        "student_training_args = DistillationTrainingArguments(\n",
        "    output_dir=finetuned_ckpt, eval_strategy = \"epoch\",\n",
        "    num_train_epochs=5, learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\n",
        "    push_to_hub=True)"
      ],
      "metadata": {
        "id": "cenXYmqV-TSy"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "student_training_args.logging_steps = len(clinc_enc['train']) // batch_size\n",
        "student_training_args.disable_tqdm = False\n",
        "student_training_args.save_steps = 1e9\n",
        "student_training_args.log_level = 40"
      ],
      "metadata": {
        "id": "npnPW7vO-TSz"
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "%env TOKENIZERS_PARALLELISM=false"
      ],
      "metadata": {
        "id": "jZJCG2f9-TSz",
        "outputId": "fec656db-f271-43a1-91d2-72432720d382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TOKENIZERS_PARALLELISM=false\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = pipe.model.config.id2label\n",
        "label2id = pipe.model.config.label2id"
      ],
      "metadata": {
        "id": "lTUb4uNp-TSz"
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "num_labels = intents.num_classes\n",
        "student_config = (AutoConfig\n",
        "                  .from_pretrained(student_ckpt, num_labels=num_labels,\n",
        "                                   id2label=id2label, label2id=label2id))"
      ],
      "metadata": {
        "id": "QIZyadBR-TSz"
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def student_init():\n",
        "    return (AutoModelForSequenceClassification\n",
        "            .from_pretrained(student_ckpt, config=student_config).to(device))"
      ],
      "metadata": {
        "id": "LiIK8XjY-TSz"
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
        "teacher_model = (AutoModelForSequenceClassification\n",
        "                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\n",
        "                 .to(device))"
      ],
      "metadata": {
        "id": "p0tJ12lf-TS0"
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_trainer = DistillationTrainer(model_init=student_init,\n",
        "    teacher_model=teacher_model, args=student_training_args,\n",
        "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
        "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
        "\n",
        "distilbert_trainer.train()"
      ],
      "metadata": {
        "id": "wOMKI6Sx-TS4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "372bc316-9aa8-4761-a294-843fcce8623e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1054698045.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "40",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3799182003.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m distilbert_trainer = DistillationTrainer(model_init=student_init,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mteacher_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_training_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclinc_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclinc_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1054698045.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, teacher_model, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDistillationTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# set the correct log level depending on the node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mlog_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process_log_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mget_process_log_level\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0;31m# convert to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m         \u001b[0mlog_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_log_levels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_level\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0mlog_level_replica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_log_levels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_level_replica\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 40"
          ]
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_trainer.push_to_hub(\"Training completed!\")"
      ],
      "metadata": {
        "id": "0ZZbDXdr-TS4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "finetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=finetuned_ckpt)"
      ],
      "metadata": {
        "id": "OnjGmyRA-TS4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "optim_type = \"DistilBERT\"\n",
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ],
      "metadata": {
        "id": "ITzB7ixX-TS4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def plot_metrics(perf_metrics, current_optim_type):\n",
        "    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n",
        "\n",
        "    for idx in df.index:\n",
        "        df_opt = df.loc[idx]\n",
        "        # Add a dashed circle around the current optimization type\n",
        "        if idx == current_optim_type:\n",
        "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
        "                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\n",
        "                        marker='$\\u25CC$')\n",
        "        else:\n",
        "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
        "                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n",
        "\n",
        "    legend = plt.legend(bbox_to_anchor=(1,1))\n",
        "    for handle in legend.legendHandles:\n",
        "        handle.set_sizes([20])\n",
        "\n",
        "    plt.ylim(80,90)\n",
        "    # Use the slowest model to define the x-axis range\n",
        "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
        "    plt.xlim(1, xlim)\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.xlabel(\"Average latency (ms)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(perf_metrics, optim_type)"
      ],
      "metadata": {
        "id": "Po7QDD_3-TS5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Good Hyperparameters with Optuna"
      ],
      "metadata": {
        "id": "JDs3euC0-TS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_input\n",
        "#id banana-function\n",
        "#alt A banana plot\n",
        "#caption Plot of the Rosenbrock function of two variables\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def f(x, y):\n",
        "    return (1-x)**2+100*(y-x**2)**2\n",
        "\n",
        "X, Y = np.meshgrid(np.linspace(-2, 2, 250), np.linspace(-1, 3, 250))\n",
        "Z = f(X,Y)\n",
        "_, ax = plt.subplots()\n",
        "ax.plot([1], [1], 'x', mew=3, markersize=10, color=\"red\")\n",
        "ax.contourf(X, Y, Z, np.logspace(-1, 3, 30), cmap='viridis', extend=\"both\")\n",
        "ax.set_xlim(-1.3, 1.3)\n",
        "ax.set_ylim(-0.9, 1.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uYu7KZw3-TS5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    x = trial.suggest_float(\"x\", -2, 2)\n",
        "    y = trial.suggest_float(\"y\", -2, 2)\n",
        "    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2"
      ],
      "metadata": {
        "id": "nMYUQxe_-TS5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "import optuna\n",
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=1000)"
      ],
      "metadata": {
        "id": "uu2nwqKY-TS5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "id": "DFq5Ani--TS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def hp_space(trial):\n",
        "    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
        "        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}"
      ],
      "metadata": {
        "id": "z6ixp1f1-TS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "best_run = distilbert_trainer.hyperparameter_search(\n",
        "    n_trials=20, direction=\"maximize\", hp_space=hp_space)"
      ],
      "metadata": {
        "id": "tP78-oYT-TS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_run)"
      ],
      "metadata": {
        "id": "toJ9WOaw-TS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "for k,v in best_run.hyperparameters.items():\n",
        "    setattr(student_training_args, k, v)\n",
        "\n",
        "# Define a new repository to store our distilled model\n",
        "distilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\n",
        "student_training_args.output_dir = distilled_ckpt\n",
        "\n",
        "# Create a new Trainer with optimal parameters\n",
        "distil_trainer = DistillationTrainer(model_init=student_init,\n",
        "    teacher_model=teacher_model, args=student_training_args,\n",
        "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
        "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
        "\n",
        "distil_trainer.train();"
      ],
      "metadata": {
        "id": "eDMp6quB-TS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "distil_trainer.push_to_hub(\"Training complete\")"
      ],
      "metadata": {
        "id": "fTY2u8OM-TS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarking Our Distilled Model"
      ],
      "metadata": {
        "id": "XZn7_iF_-TS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distilled_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=distilled_ckpt)\n",
        "optim_type = \"Distillation\"\n",
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ],
      "metadata": {
        "id": "YNXTxI4c-TS7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ],
      "metadata": {
        "id": "z1u3CV2x-TS7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Models Faster with Quantization"
      ],
      "metadata": {
        "id": "whpszY-e-TS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sidebar: A Primer on Floating-Point and Fixed-Point Numbers"
      ],
      "metadata": {
        "id": "0kuM9f0O-TS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End sidebar"
      ],
      "metadata": {
        "id": "gX6_9Oc3-TS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Mapping floating-point numbers to 8-bit integers\" width=\"800\" caption=\"Quantizing floating-point numbers as unsigned 8-bit integers (courtesy of Manas Sahni)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_fp32-to-int8.png?raw=1\" id=\"fp32toint8\"/>"
      ],
      "metadata": {
        "id": "ct9SLFLa-TS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "state_dict = pipe.model.state_dict()\n",
        "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
        "plt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZV8y_Xj0-TS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "zero_point = 0\n",
        "scale = (weights.max() - weights.min()) / (127 - (-128))"
      ],
      "metadata": {
        "id": "hXtkRAI2-TS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "(weights / scale + zero_point).clamp(-128, 127).round().char()"
      ],
      "metadata": {
        "id": "alFFZWeg-TS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import quantize_per_tensor\n",
        "\n",
        "dtype = torch.qint8\n",
        "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\n",
        "quantized_weights.int_repr()"
      ],
      "metadata": {
        "id": "WlZE1JO1-TS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_input\n",
        "#id weight-quantization\n",
        "#alt Effect of quantization on a transformer's weights\n",
        "#caption Effect of quantization on a transformer's weights\n",
        "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n",
        "\n",
        "# Create histogram\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(quantized_weights.dequantize().flatten().numpy(),\n",
        "         bins=250, range=(-0.3,0.3), edgecolor=\"C0\");\n",
        "# Create zoom inset\n",
        "axins = zoomed_inset_axes(ax, 5, loc='upper right')\n",
        "axins.hist(quantized_weights.dequantize().flatten().numpy(),\n",
        "         bins=250, range=(-0.3,0.3));\n",
        "x1, x2, y1, y2 = 0.05, 0.1, 500, 2500\n",
        "axins.set_xlim(x1, x2)\n",
        "axins.set_ylim(y1, y2)\n",
        "axins.axes.xaxis.set_visible(False)\n",
        "axins.axes.yaxis.set_visible(False)\n",
        "mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DGHh3uyW-TS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "weights @ weights"
      ],
      "metadata": {
        "id": "K8gliHIt-TS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.quantized import QFunctional\n",
        "\n",
        "q_fn = QFunctional()"
      ],
      "metadata": {
        "id": "ISX3DP3f-TS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "q_fn.mul(quantized_weights, quantized_weights)"
      ],
      "metadata": {
        "id": "DSWojiyi-TS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())"
      ],
      "metadata": {
        "id": "RY-8hSyo-TS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = (AutoModelForSequenceClassification\n",
        "         .from_pretrained(model_ckpt).to(\"cpu\"))\n",
        "\n",
        "model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"
      ],
      "metadata": {
        "id": "KiWYPBYo-TS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarking Our Quantized Model"
      ],
      "metadata": {
        "id": "AWMM8XX0-TS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-classification\", model=model_quantized,\n",
        "                tokenizer=tokenizer)\n",
        "optim_type = \"Distillation + quantization\"\n",
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ],
      "metadata": {
        "id": "8XiDZkXU-TS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ],
      "metadata": {
        "id": "_YGkgcsC-TS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing Inference with ONNX and the ONNX Runtime"
      ],
      "metadata": {
        "id": "y2MeLszQ-TS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Example ONNX graph\" width=\"500\" caption=\"A section of the ONNX graph for BERT-base, visualized in Netron\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_bert-onnx.png?raw=1\" id=\"bert-onnx\"/>"
      ],
      "metadata": {
        "id": "c3cor75r-TS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Architecture of the ONNX and ONNX Runtime ecosystem\" width=\"500\" caption=\"Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of the ONNX Runtime team)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_onnx-ort.png?raw=1\" id=\"onnx-ort\"/>"
      ],
      "metadata": {
        "id": "rvNR7Y2Q-TS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "import os\n",
        "from psutil import cpu_count\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\n",
        "os.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\""
      ],
      "metadata": {
        "id": "Ajm6KuTn-TS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "from transformers.convert_graph_to_onnx import convert\n",
        "\n",
        "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
        "onnx_model_path = Path(\"onnx/model.onnx\")\n",
        "convert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\n",
        "        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")"
      ],
      "metadata": {
        "id": "0u4sg4m0-TS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime import (GraphOptimizationLevel, InferenceSession,\n",
        "                         SessionOptions)\n",
        "\n",
        "def create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\n",
        "    options = SessionOptions()\n",
        "    options.intra_op_num_threads = 1\n",
        "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
        "    session.disable_fallback()\n",
        "    return session"
      ],
      "metadata": {
        "id": "JMd3SZjg-TS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = create_model_for_provider(onnx_model_path)"
      ],
      "metadata": {
        "id": "0BLO-8wE-TS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = clinc_enc[\"test\"][:1]\n",
        "del inputs[\"labels\"]\n",
        "logits_onnx = onnx_model.run(None, inputs)[0]\n",
        "logits_onnx.shape"
      ],
      "metadata": {
        "id": "r3KxFk5U-TS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(logits_onnx)"
      ],
      "metadata": {
        "id": "0KWVSQUT-TS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "clinc_enc[\"test\"][0][\"labels\"]"
      ],
      "metadata": {
        "id": "XiZV8Jxd-TS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class OnnxPipeline:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, query):\n",
        "        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n",
        "        inputs_onnx = {k: v.cpu().detach().numpy()\n",
        "                       for k, v in model_inputs.items()}\n",
        "        logits = self.model.run(None, inputs_onnx)[0][0, :]\n",
        "        probs = softmax(logits)\n",
        "        pred_idx = np.argmax(probs).item()\n",
        "        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]"
      ],
      "metadata": {
        "id": "uaNjUs99-TS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = OnnxPipeline(onnx_model, tokenizer)\n",
        "pipe(query)"
      ],
      "metadata": {
        "id": "GoCUZy0U-TS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class OnnxPerformanceBenchmark(PerformanceBenchmark):\n",
        "    def __init__(self, *args, model_path, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def compute_size(self):\n",
        "        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n",
        "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "        return {\"size_mb\": size_mb}"
      ],
      "metadata": {
        "id": "qLOdkyKo-TTA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "optim_type = \"Distillation + ORT\"\n",
        "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n",
        "                              model_path=\"onnx/model.onnx\")\n",
        "perf_metrics.update(pb.run_benchmark())"
      ],
      "metadata": {
        "id": "0jyzfG-a-TTA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ],
      "metadata": {
        "id": "TLxgbaP8-TTA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_input = \"onnx/model.onnx\"\n",
        "model_output = \"onnx/model.quant.onnx\"\n",
        "quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)"
      ],
      "metadata": {
        "id": "T7qE7dxS-TTB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_quantized_model = create_model_for_provider(model_output)\n",
        "pipe = OnnxPipeline(onnx_quantized_model, tokenizer)\n",
        "optim_type = \"Distillation + ORT (quantized)\"\n",
        "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n",
        "                              model_path=model_output)\n",
        "perf_metrics.update(pb.run_benchmark())"
      ],
      "metadata": {
        "id": "AcYpEnYH-TTB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(perf_metrics, optim_type)"
      ],
      "metadata": {
        "id": "59zVHWNe-TTB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Models Sparser with Weight Pruning"
      ],
      "metadata": {
        "id": "69WeejVJ-TTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparsity in Deep Neural Networks"
      ],
      "metadata": {
        "id": "DjACbL0O-TTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Network Pruning\" width=\"500\" caption=\"Weights and neurons before and after pruning (courtesy of Song Han)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_network-pruning.png?raw=1\" id=\"network-pruning\"/>"
      ],
      "metadata": {
        "id": "TvQn5oWS-TTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Pruning Methods"
      ],
      "metadata": {
        "id": "1bRa1-nQ-TTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Magnitude pruning"
      ],
      "metadata": {
        "id": "QjEsPvk2-TTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_input\n",
        "#id sparsity-scheduler\n",
        "#alt Sparsity scheduler\n",
        "#caption The cubic sparsity scheduler used for pruning\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _sparsity(t, t_0=0, dt=1, s_i=0, s_f=0.9, N=100):\n",
        "    return s_f + (s_i - s_f) * (1 - (t - t_0) / (N * dt))**3\n",
        "\n",
        "steps = np.linspace(0,100,100)\n",
        "values = [_sparsity(t) for t in steps]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps, values)\n",
        "ax.set_ylim(0,1)\n",
        "ax.set_xlim(0,100)\n",
        "ax.set_xlabel(\"Pruning step\")\n",
        "ax.set_ylabel(\"Sparsity\")\n",
        "plt.grid(linestyle=\"dashed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ucf3VydT-TTC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Movement pruning"
      ],
      "metadata": {
        "id": "To1IG_Ua-TTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Magnitude vs Movement Pruning\" width=\"700\" caption=\"Comparison of weights removed (in gray) during magnitude pruning (left) and movement pruning (right)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_magnitude-vs-movement.png?raw=1\" id=\"magnitude-vs-movement\"/>"
      ],
      "metadata": {
        "id": "46DTx2XO-TTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Pruning Distributions\" width=\"500\" caption=\"Distribution of remaining weights for magnitude pruning (MaP) and movement pruning (MvP)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_pruning-dists.png?raw=1\" id=\"pruning-dists\"/>"
      ],
      "metadata": {
        "id": "c4aa7_wI-TTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "yYdFLS0H-TTD"
      }
    }
  ]
}