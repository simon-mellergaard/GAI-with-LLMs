{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/GAI-with-LLMs/blob/main/Litterature/NLP_00_corrections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/simonmellergaard/nlp-00-corrections/edit)"
      ],
      "metadata": {
        "id": "O_ntQpfNXRfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes on Transformers Notebooks\n",
        "\n",
        "*Author: Jesper N. Wulff*\n",
        "\n",
        "This notebook contains comments and solutions to problems I encountered when running the example code [from the notebooks](https://github.com/nlp-with-transformers/notebooks) from the O'Reilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). All of the issues have been filed on GitHub."
      ],
      "metadata": {
        "id": "wWcfXC0_Z_Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01_introduction.ipynb"
      ],
      "metadata": {
        "id": "1TCkLyIhdjx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02_classification.ipynb\n",
        "\n",
        "#### **Issue 1.**\n",
        "\n",
        "The issue here pertains to the section \"From Datasets to DataFrames\". The problem arises when running this piece of code:\n",
        "\n",
        "```\n",
        "def label_int2str(row):\n",
        "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
        "\n",
        "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
        "df.head()\n",
        "```\n",
        "\n",
        "which throws the error:\n",
        "\n",
        "```\n",
        "AttributeError: 'Value' object has no attribute 'int2str'\n",
        "```\n",
        "\n",
        "One way around the problem is to simply load a newer version of the `datasets` library like [this](https://github.com/nlp-with-transformers/notebooks/issues/113#issuecomment-1727526784):\n"
      ],
      "metadata": {
        "id": "fCms4xJTdomq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "OLFd-HOje_JL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the emotions dataset\n",
        "emotions = load_dataset(\"emotion\")\n",
        "\n",
        "emotions.set_format(type=\"pandas\")\n",
        "df = emotions[\"train\"][:]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uUXr7Kk1dq6J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def label_int2str(row):\n",
        "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
        "\n",
        "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KdeSiahjfRc1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Issue 2.**\n",
        "\n",
        "To avoid a `numpy` error when calling\n",
        "\n",
        "```\n",
        "emotions_encoded.set_format(\"torch\",\n",
        "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "```\n",
        "\n",
        "run this code just before calling `map`, like [this](https://github.com/nlp-with-transformers/notebooks/issues/130#issuecomment-1923821023):\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "np.object = object\n",
        "\n",
        "#hide_output\n",
        "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bHwku-NBs9FV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Issue 3.**\n",
        "\n",
        "To avoid an error when fine-tuning the classification model, remember to create a new model on HuggingFace. For more, see [this](https://github.com/nlp-with-transformers/notebooks/issues/109#issuecomment-1712672984)."
      ],
      "metadata": {
        "id": "QVsSfYBCw7Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04_multilingual-ner.ipynb\n",
        "\n",
        "#### **Issue 1.**\n",
        "\n",
        "Under *When Does Zero-Shot Transfer Make Sense?* the following code will throw an error if `Pandas 2.0` or larger is used:\n",
        "\n",
        "```\n",
        "for num_samples in [500, 1000, 2000, 4000]:\n",
        "    metrics_df =  metrics_df.append(train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)\n",
        "```\n",
        "\n",
        "To fix this, use `concat` instead like this:\n",
        "\n",
        "```\n",
        "for num_samples in [500, 1000, 2000, 4000]:\n",
        "    metrics_df =  pd.concat([metrics_df, train_on_subset(panx_fr_encoded, num_samples)], ignore_index=True)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mX2tFLI0YO69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 05_text-generation.ipynb\n",
        "\n",
        "#### **Issue 1.**\n",
        "\n",
        "On page 127 in NLPwT, there is a mistake in footnote 3:\n",
        "\n",
        "> If you run out of memory on your machine, you can load a smaller GPT-2 version by replacing model_name = \"gpt-xl\" with model_name = \"gpt\".\n",
        "\n",
        "It is supposed to be `model_name = \"gpt2\"`\n",
        "\n"
      ],
      "metadata": {
        "id": "0dqKppPxY7mQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06_summarization.ipynb\n",
        "\n",
        "#### **Issue 1.**\n",
        "\n",
        "To avoid an error, also download `\"punkt_tab\"` from `nltk`:\n",
        "\n",
        "```\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### **Issue 2.**\n",
        "\n",
        "`load_metric` is deprecated. Alternatively, you can rely on the `evaluate` library with some changes.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "\n",
        "import evaluate\n",
        "\n",
        "rouge_metric = evaluate.load('rouge')\n",
        "```\n",
        "\n",
        "Some of the code needs to be adapted to conform to the other library. For instance, remove the suffixes from `score[rn]` in this line:\n",
        "\n",
        "```\n",
        "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AaamrNezNCFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP_08_model_compression.pynb\n",
        "\n",
        "#### **Issue 1.**\n",
        "\n",
        "Support for `load_metric` has been removed in datasets@3.0.0. Instead, we should use the `evaluate` library like this:\n",
        "\n",
        "```\n",
        "!pip install evaluate\n",
        "import evaluate\n",
        "accuracy_score = evaluate.load(\"accuracy\")\n",
        "```\n",
        "Dropping this drop-in replacement should not break the other code.\n",
        "\n",
        "\n",
        "#### **Issue 2.**\n",
        "\n",
        "When defining `time_pipeline()` you'll get a syntax warning:\n",
        "\n",
        "\n",
        "```\n",
        "<>:18: SyntaxWarning: invalid escape sequence '\\-' <>:18: SyntaxWarning:\n",
        "invalid escape sequence '\\-' /tmp/ipython-input-720322860.py:18: SyntaxWarning:\n",
        "invalid escape sequence '\\-' print(f\"Average latency (ms) - {time_avg_ms:.2f}\n",
        "+\\- {time_std_ms:.2f}\")\n",
        "```\n",
        "The warning comes from this part:\n",
        "\n",
        "```\n",
        "print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "```\n",
        "\n",
        "Python is treating `\\–` inside an f-string as an escape sequence, but `\\-' isn't valid, hence the warning.\n",
        "\n",
        "Fix it like this:\n",
        "\n",
        "```\n",
        "print(f\"Average latency (ms) - {time_avg_ms:.2f} ± {time_std_ms:.2f}\")\n",
        "```\n",
        "\n",
        "#### **Issue 3.**\n",
        "\n",
        "There are a few hick-ups in the knowledge distillation example:\n",
        "\n",
        "1. Remove `evaluation_strategy` argument from the `DistillationTrainingArguments`:\n",
        "\n",
        "```\n",
        "batch_size = 48\n",
        "\n",
        "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
        "student_training_args = DistillationTrainingArguments(\n",
        "    output_dir=finetuned_ckpt, #evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=5, learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\n",
        "    push_to_hub=True, log_level='warning', report_to = \"none\")\n",
        "```\n",
        "\n",
        "2. Modify the `DistillationTrainer` class definition to include num_items_in_batch\n",
        "\n",
        "\n",
        "```\n",
        "def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "```\n"
      ],
      "metadata": {
        "id": "GAmQI2JMtHv8"
      }
    }
  ]
}